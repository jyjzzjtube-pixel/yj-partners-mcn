# -*- coding: utf-8 -*-
"""
YJ MCN ìë™í™” ëŒ€ì‹œë³´ë“œ V3.1 â€” Flask ë°±ì—”ë“œ
============================================
ê¸°ì¡´ affiliate_system ëª¨ë“ˆì„ REST APIë¡œ ë˜í•‘.
SSEë¡œ íŒŒì´í”„ë¼ì¸ ì§„í–‰ìƒí™© ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°.

V3.1: ë³´ì•ˆ ê°•í™” (CORS ì œí•œ, ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€, ì…ë ¥ ê²€ì¦)

ì‹¤í–‰: python yj-partners-mcn/mcn_server.py
"""
import json
import os
import sys
import time
import uuid
import threading
from pathlib import Path
from queue import Queue
from datetime import datetime

from flask import Flask, request, jsonify, Response, send_file, send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_DIR = Path(__file__).parent.parent  # franchise-db/
sys.path.insert(0, str(PROJECT_DIR))
load_dotenv(PROJECT_DIR / '.env', override=True)

from affiliate_system.config import (
    GEMINI_API_KEY, ANTHROPIC_API_KEY, PEXELS_API_KEY,
    PIXABAY_API_KEY, UNSPLASH_ACCESS_KEY, RENDER_OUTPUT_DIR, WORK_DIR,
)
from affiliate_system.models import Platform, PLATFORM_PRESETS

# â”€â”€ ì»¤ë§¨ë“œì„¼í„° AI ì„œë¹„ìŠ¤ ì—°ë™ â”€â”€
from command_center.config import OPENAI_API_KEY, OLLAMA_BASE_URL, OLLAMA_MODEL, AI_PROVIDERS
from command_center.services.ai_service import AIService

ai_service = AIService()

# â”€â”€ Flask ì•± ì„¤ì • â”€â”€
app = Flask(__name__, static_folder=str(Path(__file__).parent))
CORS(app, origins=[
    "https://jyjzzjtube-pixel.github.io",
    "http://localhost:5001",
    "http://127.0.0.1:5001",
    "http://localhost:*",
])  # V3.1: í—ˆìš© origin ì œí•œ

# â”€â”€ Job ì €ì¥ì†Œ & ìº í˜ì¸ íˆìŠ¤í† ë¦¬ â”€â”€
jobs = {}  # job_id -> {status, step, progress, results, events, error}

# â”€â”€ ìº í˜ì¸ DB (SQLite) â”€â”€
import sqlite3
CAMPAIGN_DB = str(PROJECT_DIR / "mcn_campaigns.db")

def _init_campaign_db():
    """ìº í˜ì¸ íˆìŠ¤í† ë¦¬ DB ì´ˆê¸°í™”"""
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.execute("""CREATE TABLE IF NOT EXISTS campaigns (
        id TEXT PRIMARY KEY,
        topic TEXT, brand TEXT, platforms TEXT,
        ai_provider TEXT, cost_usd REAL DEFAULT 0,
        status TEXT DEFAULT 'pending',
        results TEXT,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP
    )""")
    conn.commit()
    conn.close()

_init_campaign_db()

# â”€â”€ ë¸Œëœë“œ ì„¤ì • â”€â”€
BRANDS = {
    "ì˜¤ë ˆë…¸ì¹´ì¸ ": {
        "name": "ì˜¤ë ˆë…¸ì¹´ì¸ ",
        "type": "Japanese Tonkatsu Franchise",
        "emoji": "ğŸ±",
        "platforms": ["youtube", "naver_blog", "instagram"],
        "campaigns": 12,
    },
    "ë¬´ì‚¬ì§¬ë½•": {
        "name": "ë¬´ì‚¬ì§¬ë½•",
        "type": "Chinese Jjamppong Franchise",
        "emoji": "ğŸœ",
        "platforms": ["youtube", "naver_blog", "instagram"],
        "campaigns": 8,
    },
    "ë¸Œë¦¿ì§€ì›": {
        "name": "ë¸Œë¦¿ì§€ì›",
        "type": "Franchise Consulting (BRIDGE ONE)",
        "emoji": "ğŸ’¼",
        "platforms": ["youtube", "naver_blog", "instagram", "coupang"],
        "campaigns": 15,
    },
}

PLATFORM_MAP = {
    "youtube": Platform.YOUTUBE,
    "instagram": Platform.INSTAGRAM,
    "naver_blog": Platform.NAVER_BLOG,
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _to_relative_path(abs_path) -> str:
    """ì ˆëŒ€ ê²½ë¡œë¥¼ PROJECT_DIR ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œ íŒŒì¼ ì„œë¹™ìš©)."""
    if not abs_path:
        return ""
    p = Path(str(abs_path))
    try:
        return str(p.relative_to(PROJECT_DIR)).replace("\\", "/")
    except ValueError:
        # PROJECT_DIR ë°–ì˜ ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return str(p).replace("\\", "/")



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WebPipeline â€” ë‹¨ê³„ë³„ SSE ì´ë²¤íŠ¸ ë°œìƒ ë˜í¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class WebPipeline:
    """ContentPipelineì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•˜ë©° SSE ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œí‚¨ë‹¤."""

    def __init__(self, events_queue: Queue):
        self._q = events_queue

    def _emit(self, step: int, name: str, status: str, detail: str = ""):
        self._q.put({
            "type": "step",
            "step": step,
            "name": name,
            "status": status,
            "detail": detail,
            "timestamp": datetime.now().isoformat(),
        })

    def run(self, topic: str, platforms: list, brand: str, persona: str,
            auto_upload: bool, drive_archive: bool = True) -> dict:
        from affiliate_system.pipeline import ContentPipeline

        pipeline = ContentPipeline()
        platform_enums = [PLATFORM_MAP[p] for p in platforms if p in PLATFORM_MAP]
        if not platform_enums:
            platform_enums = list(PLATFORM_MAP.values())

        results = {"platforms": {}, "upload_results": {}}

        # Step 1: ìƒí’ˆ ì •ë³´
        self._emit(1, "product_info", "running", "ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        try:
            product = pipeline._prepare_product(topic)
            self._emit(1, "product_info", "complete", f"ìƒí’ˆ: {product.title}")
        except Exception as e:
            self._emit(1, "product_info", "error", str(e))
            raise

        # Step 2: AI ì½˜í…ì¸  ìƒì„±
        self._emit(2, "ai_content", "running", f"{len(platform_enums)}ê°œ í”Œë«í¼ AI ì½˜í…ì¸  ìƒì„± ì¤‘...")
        try:
            platform_contents = pipeline._generate_contents(product, platform_enums, persona, brand)
            detail_parts = []
            for p_name, content in platform_contents.items():
                narr = len(content.get("narration", []))
                tags = len(content.get("hashtags", []))
                detail_parts.append(f"{p_name}: ë‚˜ë ˆì´ì…˜ {narr}ì¥ë©´, íƒœê·¸ {tags}ê°œ")
            self._emit(2, "ai_content", "complete", " | ".join(detail_parts))
        except Exception as e:
            self._emit(2, "ai_content", "error", str(e))
            raise

        # Step 3: ë¯¸ë””ì–´ ìˆ˜ì§‘
        self._emit(3, "media", "running", "ë¬´ë£Œ ìŠ¤í†¡ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
        try:
            images = pipeline._collect_media(product)
            self._emit(3, "media", "complete", f"ì´ë¯¸ì§€ {len(images)}ê°œ ìˆ˜ì§‘")
        except Exception as e:
            self._emit(3, "media", "error", str(e))
            images = []

        # Step 4: ì¸ë„¤ì¼ ìƒì„±
        self._emit(4, "thumbnail", "running", "í”Œë«í¼ë³„ ì¸ë„¤ì¼ ìƒì„± ì¤‘...")
        try:
            campaign_id = uuid.uuid4().hex[:8]
            thumbnails = pipeline._generate_thumbnails(
                platform_enums, platform_contents, images, brand, campaign_id,
            )
            self._emit(4, "thumbnail", "complete", f"{len(thumbnails)}ê°œ ì¸ë„¤ì¼ ìƒì„±")
        except Exception as e:
            self._emit(4, "thumbnail", "error", str(e))
            thumbnails = {}

        # Step 5: ì˜ìƒ ë Œë”ë§
        self._emit(5, "video_render", "running", "ì˜ìƒ ë Œë”ë§ ì¤‘ (ì‹œê°„ ì†Œìš”)...")
        try:
            videos = pipeline._render_videos(
                platform_enums, platform_contents, images, brand, campaign_id,
            )
            rendered = [p for p, v in videos.items() if v]
            self._emit(5, "video_render", "complete", f"{len(rendered)}ê°œ ì˜ìƒ ë Œë”ë§ ì™„ë£Œ")
        except Exception as e:
            self._emit(5, "video_render", "error", str(e))
            videos = {}

        # ê²°ê³¼ ì¡°í•© (ì ˆëŒ€ ê²½ë¡œ â†’ ìƒëŒ€ ê²½ë¡œ ë³€í™˜)
        for p in platform_enums:
            p_name = p.value
            results["platforms"][p_name] = {
                "video": _to_relative_path(videos.get(p_name)) or None,
                "thumbnail": _to_relative_path(thumbnails.get(p_name)) or None,
                "content": platform_contents.get(p_name, {}),
            }

        # Step 6: ì†Œì…œ ë¯¸ë””ì–´ ì—…ë¡œë“œ
        if auto_upload:
            self._emit(6, "upload", "running", "3í”Œë«í¼ ì—…ë¡œë“œ ì¤‘...")
            try:
                from affiliate_system.models import Campaign, AIContent, CampaignStatus
                campaign_obj = Campaign(
                    id=campaign_id, product=product,
                    ai_content=AIContent(platform_contents=platform_contents),
                    status=CampaignStatus.UPLOADING,
                    target_platforms=platform_enums,
                    platform_videos=videos, platform_thumbnails=thumbnails,
                    created_at=datetime.now(),
                )
                upload_results = pipeline._upload_all(campaign_obj)
                results["upload_results"] = upload_results
                self._emit(6, "upload", "complete", "ì—…ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                self._emit(6, "upload", "error", str(e))
        else:
            self._emit(6, "upload", "skipped", "ì—…ë¡œë“œ ê±´ë„ˆëœ€ (ìˆ˜ë™ ëª¨ë“œ)")

        # Step 7: Google Drive ìë™ ì•„ì¹´ì´ë¹™
        if drive_archive:
            self._emit(7, "drive_archive", "running", "Google Drive í´ë” ìƒì„± ë° ì—…ë¡œë“œ ì¤‘...")
            try:
                from affiliate_system.drive_manager import DriveArchiver
                from affiliate_system.models import Campaign, AIContent, CampaignStatus

                # Campaign ê°ì²´ ìƒì„±
                campaign_obj = Campaign(
                    id=campaign_id, product=product,
                    ai_content=AIContent(platform_contents=platform_contents),
                    status=CampaignStatus.COMPLETE,
                    target_platforms=platform_enums,
                    platform_videos=videos, platform_thumbnails=thumbnails,
                    created_at=datetime.now(),
                )

                # ì—…ë¡œë“œí•  íŒŒì¼ ë¶„ë¥˜
                drive_files = {"images": [], "renders": [], "audio": [], "logs": []}

                # ë Œë”ë§ ê²°ê³¼ (ì˜ìƒ + ì¸ë„¤ì¼)
                for p_name, v_path in videos.items():
                    if v_path and Path(str(v_path)).exists():
                        drive_files["renders"].append(str(v_path))
                for p_name, t_path in thumbnails.items():
                    if t_path and Path(str(t_path)).exists():
                        drive_files["renders"].append(str(t_path))

                # ìˆ˜ì§‘ëœ ì´ë¯¸ì§€
                media_dir = WORK_DIR / "media_downloads"
                if media_dir.exists():
                    for img in sorted(media_dir.glob("*.*"))[-20:]:  # ìµœê·¼ 20ê°œ
                        if img.suffix.lower() in (".jpg", ".jpeg", ".png", ".webp"):
                            drive_files["images"].append(str(img))

                # TTS ì˜¤ë””ì˜¤ (ìµœê·¼ ìƒì„±ëœ tts_ í´ë”)
                for tts_dir in sorted(WORK_DIR.glob("tts_*"), reverse=True)[:1]:
                    if tts_dir.is_dir():
                        for audio_f in tts_dir.glob("*.mp3"):
                            drive_files["audio"].append(str(audio_f))

                total_files = sum(len(v) for v in drive_files.values())
                self._emit(7, "drive_archive", "running",
                           f"{total_files}ê°œ íŒŒì¼ Drive ì—…ë¡œë“œ ì¤‘...")

                # DriveArchiverë¡œ ì—…ë¡œë“œ
                archiver = DriveArchiver()
                if archiver.authenticate():
                    def _progress(cur, tot, fname):
                        self._emit(7, "drive_archive", "running",
                                   f"Drive ì—…ë¡œë“œ {cur}/{tot}: {fname}")

                    archive_result = archiver.archive_campaign(
                        campaign_obj, drive_files, progress_callback=_progress
                    )

                    if archive_result["ok"]:
                        folder_url = archive_result.get("folder_url", "")
                        uploaded = archive_result["files_uploaded"]
                        self._emit(7, "drive_archive", "complete",
                                   f"Drive ì•„ì¹´ì´ë¹™ ì™„ë£Œ: {uploaded}ê°œ íŒŒì¼ ì—…ë¡œë“œ")
                        results["drive_url"] = folder_url
                        results["drive_files_uploaded"] = uploaded
                    else:
                        errors = archive_result.get("errors", [])
                        self._emit(7, "drive_archive", "error",
                                   f"ì¼ë¶€ ì‹¤íŒ¨: {', '.join(errors[:3])}")
                else:
                    self._emit(7, "drive_archive", "error",
                               "Drive ì¸ì¦ ì‹¤íŒ¨ â€” OAuth í† í°ì„ í™•ì¸í•˜ì„¸ìš”")
            except Exception as e:
                self._emit(7, "drive_archive", "error", str(e))
        else:
            self._emit(7, "drive_archive", "skipped", "Drive ì•„ì¹´ì´ë¹™ ê±´ë„ˆëœ€")

        results["campaign_id"] = campaign_id
        return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ëª¨ë¦¬ ê´€ë¦¬ â€” ì˜¤ë˜ëœ ì¡ ìë™ ì •ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _cleanup_old_jobs(jobs_dict, max_age_seconds=3600):
    """1ì‹œê°„ ì´ìƒ ëœ ì™„ë£Œ/ì—ëŸ¬ ì¡ì„ ì œê±°í•˜ì—¬ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€."""
    now = datetime.now()
    to_remove = []
    active_states = ("running", "pending", "analyzing", "awaiting_confirm", "executing")
    for jid, job in jobs_dict.items():
        created = datetime.fromisoformat(job.get("created_at", now.isoformat()))
        status = job.get("status", job.get("state", ""))
        if (now - created).total_seconds() > max_age_seconds and status not in active_states:
            to_remove.append(jid)
    for jid in to_remove:
        # V3 íŒŒì´í”„ë¼ì¸ ê°ì²´ ì°¸ì¡° í•´ì œ (ë©”ëª¨ë¦¬ í™•ë³´)
        job = jobs_dict[jid]
        if "pipeline" in job:
            del job["pipeline"]
        del jobs_dict[jid]


def _start_periodic_cleanup():
    """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ: ëª¨ë“  ì¡ ì €ì¥ì†Œë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì •ë¦¬ (ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œ)."""
    def _loop():
        while True:
            time.sleep(600)  # 10ë¶„ë§ˆë‹¤ ì‹¤í–‰
            try:
                _cleanup_old_jobs(jobs)
                if 'v2_jobs' in globals():
                    _cleanup_old_jobs(v2_jobs)
                if 'v3_jobs' in globals():
                    _cleanup_old_jobs(v3_jobs)
            except Exception:
                pass
    threading.Thread(target=_loop, daemon=True).start()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ ë©”ì¸ í˜ì´ì§€ â”€â”€
@app.route('/')
def index():
    return send_file(str(Path(__file__).parent / 'index.html'))


# â”€â”€ ê±´ê°• ì²´í¬ (AI 8ê°œ + ë¯¸ë””ì–´ + OpenClaw) â”€â”€
@app.route('/api/health')
def health():
    # AI í”„ë¡œë°”ì´ë” ìƒíƒœ
    providers = ai_service.list_providers()
    services = {}
    for p in providers:
        services[p["name"]] = p["available"]
    # ë¯¸ë””ì–´ API
    services["pexels"] = bool(PEXELS_API_KEY)
    services["pixabay"] = bool(PIXABAY_API_KEY)
    services["unsplash"] = bool(UNSPLASH_ACCESS_KEY)
    # OpenClaw ê²Œì´íŠ¸ì›¨ì´
    try:
        import requests as req
        oc = req.get("http://127.0.0.1:18792/__openclaw__/health", timeout=2)
        services["openclaw"] = oc.status_code == 200
    except Exception:
        services["openclaw"] = False

    return jsonify({
        "status": "online",
        "services": services,
        "active_jobs": sum(1 for j in jobs.values() if j["status"] == "running"),
        "ai_providers": providers,
        "timestamp": datetime.now().isoformat(),
    })


# â”€â”€ ë¸Œëœë“œ ëª©ë¡ â”€â”€
@app.route('/api/brands')
def get_brands():
    return jsonify(BRANDS)


# â”€â”€ ìº í˜ì¸ ì‹œì‘ â”€â”€
@app.route('/api/campaign/start', methods=['POST'])
def start_campaign():
    _cleanup_old_jobs(jobs)  # ì˜¤ë˜ëœ ì¡ ì •ë¦¬

    data = request.json or {}
    topic = data.get("topic", "").strip()
    if not topic:
        return jsonify({"error": "topic í•„ìˆ˜"}), 400

    brand = data.get("brand", "")
    platforms = data.get("platforms", ["youtube", "instagram", "naver_blog"])
    persona = data.get("persona", "")
    auto_upload = data.get("auto_upload", False)
    drive_archive = data.get("drive_archive", True)  # ê¸°ë³¸ ON

    job_id = uuid.uuid4().hex[:12]
    events_queue = Queue()

    jobs[job_id] = {
        "status": "pending",
        "step": 0,
        "topic": topic,
        "brand": brand,
        "platforms": platforms,
        "results": None,
        "error": None,
        "events": events_queue,
        "created_at": datetime.now().isoformat(),
    }

    # ìº í˜ì¸ íˆìŠ¤í† ë¦¬ ì €ì¥ (ì‹œì‘)
    _save_campaign(job_id, topic, brand, platforms, "running")

    def worker():
        job = jobs[job_id]
        job["status"] = "running"
        try:
            wp = WebPipeline(events_queue)
            results = wp.run(topic, platforms, brand, persona, auto_upload, drive_archive)
            job["results"] = results
            job["status"] = "complete"
            events_queue.put({"type": "complete", "results": results})
            # ìº í˜ì¸ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ì™„ë£Œ)
            _save_campaign(job_id, topic, brand, platforms, "complete",
                           results=_safe_serialize(results))
        except Exception as e:
            job["error"] = str(e)
            job["status"] = "error"
            events_queue.put({"type": "error", "error": str(e)})
            _save_campaign(job_id, topic, brand, platforms, "error")

    thread = threading.Thread(target=worker, daemon=True)
    thread.start()

    return jsonify({"job_id": job_id, "status": "started"})


# â”€â”€ SSE ì§„í–‰ìƒí™© ìŠ¤íŠ¸ë¦¬ë° â”€â”€
@app.route('/api/campaign/stream/<job_id>')
def stream_campaign(job_id):
    def generate():
        job = jobs.get(job_id)
        if not job:
            yield f"data: {json.dumps({'type': 'error', 'error': 'Job not found'})}\n\n"
            return

        q = job["events"]
        while job["status"] in ("pending", "running"):
            while True:
                try:
                    event = q.get_nowait()
                except Exception:
                    break
                # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
                if event.get("type") == "complete" and event.get("results"):
                    event["results"] = _safe_serialize(event["results"])
                yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"
            time.sleep(0.3)

        # ì”ì—¬ ì´ë²¤íŠ¸ flush
        while True:
            try:
                event = q.get_nowait()
            except Exception:
                break
            if event.get("type") == "complete" and event.get("results"):
                event["results"] = _safe_serialize(event["results"])
            yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"

        # ìµœì¢… ìƒíƒœ
        if job["status"] == "complete" and job["results"]:
            yield f"data: {json.dumps({'type': 'done', 'results': _safe_serialize(job['results'])}, ensure_ascii=False, default=str)}\n\n"
        elif job["status"] == "error":
            yield f"data: {json.dumps({'type': 'error', 'error': job['error']})}\n\n"

    return Response(generate(), mimetype='text/event-stream',
                    headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})


def _safe_serialize(obj):
    """ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜."""
    if isinstance(obj, dict):
        return {k: _safe_serialize(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_safe_serialize(v) for v in obj]
    if isinstance(obj, Path):
        return str(obj)
    if hasattr(obj, '__dict__') and not isinstance(obj, type):
        return str(obj)
    return obj


# â”€â”€ AI ì½˜í…ì¸ ë§Œ ìƒì„± (ì˜ìƒ ì—†ì´) â”€â”€
@app.route('/api/content/generate', methods=['POST'])
def generate_content():
    data = request.json or {}
    topic = data.get("topic", "").strip()
    if not topic:
        return jsonify({"error": "topic í•„ìˆ˜"}), 400

    brand = data.get("brand", "")
    persona = data.get("persona", "")
    platforms = data.get("platforms", ["youtube", "instagram", "naver_blog"])

    try:
        from affiliate_system.pipeline import ContentPipeline
        pipeline = ContentPipeline()
        product = pipeline._prepare_product(topic)

        platform_enums = [PLATFORM_MAP[p] for p in platforms if p in PLATFORM_MAP]
        contents = pipeline._generate_contents(product, platform_enums, persona, brand)

        return jsonify({
            "product": {"title": product.title, "description": product.description},
            "contents": contents,
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ë¬´ë£Œ ë¯¸ë””ì–´ ê²€ìƒ‰ â”€â”€
@app.route('/api/media/search', methods=['POST'])
def search_media():
    data = request.json or {}
    query = data.get("query", "").strip()
    media_type = data.get("type", "image")  # image or video

    if not query:
        return jsonify({"error": "query í•„ìˆ˜"}), 400

    try:
        from affiliate_system.media_collector import MediaCollector
        mc = MediaCollector()

        if media_type == "video":
            results = mc.search_videos(query, count=6)
        else:
            results = mc.search_images(query, count=12)

        return jsonify({"results": results, "query": query, "type": media_type})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ì†Œì…œ ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ (TikTok, YouTube ë“±) â”€â”€
@app.route('/api/media/social', methods=['POST'])
def download_social():
    data = request.json or {}
    url = data.get("url", "").strip()

    if not url:
        return jsonify({"error": "url í•„ìˆ˜"}), 400

    try:
        from affiliate_system.media_collector import MediaCollector
        mc = MediaCollector()
        result = mc.download_from_social(url)
        return jsonify({"result": result})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ AI ì´ë¯¸ì§€ ìƒì„± (Gemini) â”€â”€
@app.route('/api/media/ai-generate', methods=['POST'])
def ai_generate():
    data = request.json or {}
    prompt = data.get("prompt", "").strip()
    media_type = data.get("type", "image")

    if not prompt:
        return jsonify({"error": "prompt í•„ìˆ˜"}), 400

    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)

        if media_type == "image":
            # Gemini ì´ë¯¸ì§€ ìƒì„±
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            response = model.generate_content(prompt)
            # í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
            return jsonify({
                "type": "image",
                "prompt_used": prompt,
                "response": response.text if response.text else "ìƒì„± ì™„ë£Œ",
            })
        else:
            return jsonify({"error": "video AI ìƒì„±ì€ ì¤€ë¹„ ì¤‘"}), 501
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ë¹„ìš© í˜„í™© (ì»¤ë§¨ë“œì„¼í„° ì—°ë™) â”€â”€
@app.route('/api/cost')
def get_cost():
    try:
        from api_cost_tracker import CostTracker
        tracker = CostTracker()
        summary = tracker.get_summary()
    except Exception:
        summary = {"total_usd": 0}

    # AI í”„ë¡œë°”ì´ë”ë³„ ë¹„ìš© ì •ë³´
    summary["providers"] = {}
    for name, info in AI_PROVIDERS.items():
        summary["providers"][name] = {
            "model": info["model"],
            "cost_tier": info["cost"],
        }
    return jsonify(summary)


# â”€â”€ AI í”„ë¡œë°”ì´ë” ëª©ë¡ â”€â”€
@app.route('/api/ai/providers')
def ai_providers():
    return jsonify(ai_service.list_providers())


# â”€â”€ AI ì§ì ‘ í˜¸ì¶œ (í…ŒìŠ¤íŠ¸/ë‹¨ë… ì‚¬ìš©) â”€â”€
@app.route('/api/ai/ask', methods=['POST'])
def ai_ask():
    data = request.json or {}
    prompt = data.get("prompt", "").strip()
    provider = data.get("provider")  # Noneì´ë©´ ìë™ í´ë°±
    if not prompt:
        return jsonify({"error": "prompt í•„ìˆ˜"}), 400

    try:
        resp = ai_service.ask(prompt, provider=provider)
        return jsonify({
            "text": resp.text,
            "provider": resp.provider,
            "model": resp.model,
            "tokens": {"input": resp.input_tokens, "output": resp.output_tokens},
            "cost_usd": resp.cost_usd,
            "elapsed_ms": resp.elapsed_ms,
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ìº í˜ì¸ íˆìŠ¤í† ë¦¬ â”€â”€
@app.route('/api/campaigns')
def list_campaigns():
    """ìµœê·¼ ìº í˜ì¸ ì´ë ¥ ì¡°íšŒ"""
    limit = request.args.get("limit", 20, type=int)
    limit = min(limit, 100)  # ìµœëŒ€ 100ê°œë¡œ ì œí•œ
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.row_factory = sqlite3.Row
    rows = conn.execute(
        "SELECT * FROM campaigns ORDER BY created_at DESC LIMIT ?", (limit,)
    ).fetchall()
    conn.close()
    return jsonify([dict(r) for r in rows])


@app.route('/api/campaigns/<campaign_id>')
def get_campaign(campaign_id):
    """íŠ¹ì • ìº í˜ì¸ ìƒì„¸ ì¡°íšŒ"""
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.row_factory = sqlite3.Row
    row = conn.execute("SELECT * FROM campaigns WHERE id = ?", (campaign_id,)).fetchone()
    conn.close()
    if not row:
        return jsonify({"error": "ìº í˜ì¸ ì—†ìŒ"}), 404
    result = dict(row)
    # ê²°ê³¼ JSON íŒŒì‹±
    if result.get("results"):
        try:
            result["results"] = json.loads(result["results"])
        except Exception:
            pass
    return jsonify(result)


def _save_campaign(campaign_id, topic, brand, platforms, status, results=None, cost=0.0):
    """ìº í˜ì¸ ì´ë ¥ DB ì €ì¥"""
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.execute("""INSERT OR REPLACE INTO campaigns
        (id, topic, brand, platforms, status, results, cost_usd, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
        (campaign_id, topic, brand, json.dumps(platforms),
         status, json.dumps(results) if results else None,
         cost, datetime.now().isoformat())
    )
    conn.commit()
    conn.close()


# â”€â”€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ë¯¸ë¦¬ë³´ê¸° (ë Œë”ë§ëœ ì˜ìƒ/ì´ë¯¸ì§€) â”€â”€
@app.route('/api/file/<path:filepath>')
def serve_file(filepath):
    full_path = (PROJECT_DIR / filepath).resolve()
    # ê²½ë¡œ ì´íƒˆ ë°©ì§€ (path traversal ì°¨ë‹¨)
    if not str(full_path).startswith(str(PROJECT_DIR.resolve())):
        return jsonify({"error": "ì ‘ê·¼ ê±°ë¶€"}), 403
    if full_path.exists() and full_path.is_file():
        # MIME íƒ€ì… ìë™ ê°ì§€ + ë¹„ë””ì˜¤/ì´ë¯¸ì§€ëŠ” inline í‘œì‹œ
        suffix = full_path.suffix.lower()
        mime_map = {
            '.mp4': 'video/mp4', '.webm': 'video/webm', '.avi': 'video/x-msvideo',
            '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png',
            '.gif': 'image/gif', '.webp': 'image/webp',
        }
        mimetype = mime_map.get(suffix)
        return send_file(str(full_path), mimetype=mimetype)
    return jsonify({"error": "íŒŒì¼ ì—†ìŒ"}), 404


# â”€â”€ ë Œë”ë§ ì¶œë ¥ í´ë” ì§ì ‘ ëª©ë¡ (ë””ë²„ê¹…ìš©) â”€â”€
@app.route('/api/renders')
def list_renders():
    renders_dir = PROJECT_DIR / "affiliate_system" / "renders"
    if not renders_dir.exists():
        return jsonify({"files": []})
    files = []
    for f in sorted(renders_dir.iterdir(), key=lambda x: x.stat().st_mtime, reverse=True):
        if f.is_file():
            files.append({
                "name": f.name,
                "size_mb": round(f.stat().st_size / (1024*1024), 2),
                "url": f"/api/file/affiliate_system/renders/{f.name}",
                "modified": datetime.fromtimestamp(f.stat().st_mtime).isoformat(),
            })
    return jsonify({"files": files[:50]})


# â”€â”€ Google Drive ìƒíƒœ â”€â”€
@app.route('/api/drive/status')
def drive_status():
    """Google Drive ì¸ì¦ ìƒíƒœ ë° ì €ì¥ìš©ëŸ‰ í™•ì¸"""
    try:
        from affiliate_system.drive_manager import DriveArchiver
        archiver = DriveArchiver()
        token_path = archiver.TOKEN_PATH
        token_exists = token_path.exists()

        if token_exists and archiver.authenticate():
            usage = archiver.get_storage_usage()
            return jsonify({
                "authenticated": True,
                "token_path": str(token_path),
                "storage": usage,
            })
        else:
            return jsonify({
                "authenticated": False,
                "token_path": str(token_path),
                "token_exists": token_exists,
                "error": "ì¸ì¦ í•„ìš”" if not token_exists else "í† í° ë§Œë£Œ",
            })
    except Exception as e:
        return jsonify({"authenticated": False, "error": str(e)})


@app.route('/api/drive/campaigns')
def drive_campaigns():
    """Google Driveì— ì•„ì¹´ì´ë¹™ëœ ìº í˜ì¸ ëª©ë¡"""
    try:
        from affiliate_system.drive_manager import DriveArchiver
        archiver = DriveArchiver()
        if archiver.authenticate():
            campaigns = archiver.list_campaigns()
            return jsonify({"campaigns": campaigns})
        return jsonify({"error": "Drive ì¸ì¦ ì‹¤íŒ¨"}), 401
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ìº í˜ì¸ ìƒíƒœ ì¡°íšŒ â”€â”€
@app.route('/api/campaign/status/<job_id>')
def campaign_status(job_id):
    job = jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404

    resp = {
        "job_id": job_id,
        "status": job["status"],
        "topic": job["topic"],
        "brand": job["brand"],
        "platforms": job["platforms"],
        "created_at": job["created_at"],
        "error": job.get("error"),
    }
    if job["status"] == "complete" and job["results"]:
        resp["results"] = _safe_serialize(job["results"])
    return jsonify(resp)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# V2 â€” ëŒ€í™”í˜• ì¿ íŒ¡ ìˆ˜ìµ ê·¹ëŒ€í™” íŒŒì´í”„ë¼ì¸ API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# V2 Job ì €ì¥ì†Œ (interactive ìƒíƒœë¨¸ì‹ )
v2_jobs = {}  # job_id -> {state, coupang_link, product, draft, events, ...}


class V2PipelineState:
    """V2 ëŒ€í™”í˜• íŒŒì´í”„ë¼ì¸ ìƒíƒœ enum."""
    IDLE = "idle"
    AWAITING_LINK = "awaiting_link"
    ANALYZING = "analyzing"
    AWAITING_CONFIRM = "awaiting_confirm"
    EXECUTING = "executing"
    COMPLETE = "complete"
    ERROR = "error"


# V2 10ë‹¨ê³„ ì •ì˜
V2_STEPS = [
    {"step": 1,  "name": "prep_report",    "label": "ì¤€ë¹„ ë¦¬í¬íŠ¸",           "module": "pipeline"},
    {"step": 2,  "name": "link_analysis",   "label": "ì¿ íŒ¡ ë§í¬ ë¶„ì„",       "module": "coupang_scraper"},
    {"step": 3,  "name": "ai_content",      "label": "AI ì½˜í…ì¸  ìƒì„±",       "module": "ai_generator V2"},
    {"step": 4,  "name": "media_crawl",     "label": "ë¯¸ë””ì–´ í¬ë¡¤ë§",         "module": "OmniMediaCollector"},
    {"step": 5,  "name": "blog_compose",    "label": "ë¸”ë¡œê·¸ HTML ì¡°ë¦½",      "module": "blog_html_generator"},
    {"step": 6,  "name": "video_launder",   "label": "4ë‹¨ê³„ ì˜ìƒ ì„¸íƒ",       "module": "video_launderer (FFmpeg GPU)"},
    {"step": 7,  "name": "shorts_render",   "label": "ìˆí¼ ë Œë”ë§",          "module": "ShortsRenderer + TTS + Whisper"},
    {"step": 8,  "name": "thumbnail",       "label": "ì¸ë„¤ì¼ ìƒì„±",          "module": "thumbnail_generator"},
    {"step": 9,  "name": "upload_ready",    "label": "ì—…ë¡œë“œ ì¤€ë¹„",          "module": "auto_uploader V2"},
    {"step": 10, "name": "drive_archive",   "label": "Drive ì•„ì¹´ì´ë¹™",       "module": "drive_manager"},
]


@app.route('/api/v2/steps')
def v2_steps():
    """V2 10ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì •ì˜ ë°˜í™˜."""
    return jsonify(V2_STEPS)


@app.route('/api/v2/campaign/start', methods=['POST'])
def v2_start_campaign():
    """V2 ëŒ€í™”í˜• ìº í˜ì¸ ì‹œì‘ â€” "ì¿ íŒ¡ ë§í¬ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”" ìƒíƒœë¡œ ì§„ì…."""
    _cleanup_old_jobs(v2_jobs)  # ì˜¤ë˜ëœ ì¡ ì •ë¦¬

    job_id = uuid.uuid4().hex[:12]
    events_queue = Queue()

    v2_jobs[job_id] = {
        "state": V2PipelineState.AWAITING_LINK,
        "coupang_link": None,
        "product_info": None,
        "draft": None,
        "blog_html": None,
        "shorts_script": None,
        "results": {},
        "error": None,
        "events": events_queue,
        "created_at": datetime.now().isoformat(),
        "platforms": ["naver_blog", "youtube", "instagram"],
    }

    events_queue.put({
        "type": "state_change",
        "state": V2PipelineState.AWAITING_LINK,
        "message": "ğŸ”— ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”!",
        "timestamp": datetime.now().isoformat(),
    })

    return jsonify({
        "job_id": job_id,
        "state": V2PipelineState.AWAITING_LINK,
        "message": "ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”",
    })


@app.route('/api/v2/campaign/<job_id>/link', methods=['POST'])
def v2_submit_link(job_id):
    """V2 ì¿ íŒ¡ ë§í¬ ì œì¶œ â†’ ìƒí’ˆ ë¶„ì„ ì‹œì‘."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    if job["state"] != V2PipelineState.AWAITING_LINK:
        return jsonify({"error": f"í˜„ì¬ ìƒíƒœ: {job['state']}, ë§í¬ ì…ë ¥ ë¶ˆê°€"}), 400

    data = request.json or {}
    coupang_link = data.get("coupang_link", "").strip()       # ìƒí’ˆì •ë³´ URL (ìŠ¤í¬ë˜í•‘ìš©)
    affiliate_link = data.get("affiliate_link", "").strip()   # ë‹¨ì¶• URL (ìˆ˜ìµ ë§í¬)
    banner_tag = data.get("banner_tag", "").strip()           # ì¿ íŒ¡ ë°°ë„ˆ ì½”ë“œ (<a><img> ë˜ëŠ” iframe)
    product_name = data.get("product_name", "").strip()

    app.logger.debug(f"[SUBMIT] coupang_link={coupang_link[:80]}")
    app.logger.debug(f"[SUBMIT] affiliate_link={affiliate_link}")
    app.logger.debug(f"[SUBMIT] banner_tag_len={len(banner_tag)}")
    app.logger.debug(f"[SUBMIT] product_name_input={product_name}")

    # ë°°ë„ˆì½”ë“œ alt ì†ì„±ì—ì„œ ìƒí’ˆëª… ìë™ ì¶”ì¶œ (ì‚¬ìš©ìê°€ ìƒí’ˆëª… ë¯¸ì…ë ¥ ì‹œ)
    if not product_name and banner_tag:
        import re as _re
        _alt_match = _re.search(r'alt=["\']([^"\']+)["\']', banner_tag)
        if _alt_match:
            product_name = _alt_match.group(1).strip()
            app.logger.debug(f"[ALT_EXTRACT] product_name={product_name}")
        else:
            app.logger.debug("[ALT_EXTRACT] NO MATCH in banner_tag")
    elif product_name:
        app.logger.debug(f"[ALT_EXTRACT] SKIPPED - product_name already set: {product_name}")
    else:
        app.logger.debug("[ALT_EXTRACT] SKIPPED - no banner_tag")

    if not coupang_link:
        return jsonify({"error": "ìƒí’ˆì •ë³´ ë§í¬ í•„ìˆ˜"}), 400
    if not affiliate_link:
        return jsonify({"error": "ë‹¨ì¶• URL í•„ìˆ˜"}), 400

    job["coupang_link"] = coupang_link
    job["affiliate_link"] = affiliate_link
    job["banner_tag"] = banner_tag
    job["product_name"] = product_name
    job["state"] = V2PipelineState.ANALYZING
    job["events"].put({
        "type": "state_change",
        "state": V2PipelineState.ANALYZING,
        "message": "ğŸ” ìƒí’ˆ ë¶„ì„ ì¤‘...",
        "timestamp": datetime.now().isoformat(),
    })

    # ë¹„ë™ê¸° ë¶„ì„
    def analyze():
        try:
            # Step 1: ì¤€ë¹„
            job["events"].put({
                "type": "v2_step", "step": 1, "name": "prep_report",
                "status": "complete", "detail": "V2 íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ",
                "timestamp": datetime.now().isoformat(),
            })

            # Step 2: ë§í¬ ë¶„ì„
            job["events"].put({
                "type": "v2_step", "step": 2, "name": "link_analysis",
                "status": "running", "detail": "ì¿ íŒ¡ ë§í¬ ìŠ¤í¬ë˜í•‘ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })

            from affiliate_system.pipeline import ContentPipeline
            from affiliate_system.models import Product
            pipeline = ContentPipeline()
            product = pipeline._prepare_product(coupang_link)

            # ë””ë²„ê·¸ ë¡œê·¸ â€” ìŠ¤í¬ë˜í•‘ ê²°ê³¼ + í´ë°± íŒë‹¨
            app.logger.debug(f"[SCRAPE] product.title={product.title}")
            app.logger.debug(f"[SCRAPE] product.description={str(product.description)[:100]}")
            app.logger.debug(f"[SCRAPE] product_name_var={product_name}")

            # ì¿ íŒ¡ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ë°°ë„ˆì½”ë“œ alt â†’ ì‚¬ìš©ì ì…ë ¥ ìƒí’ˆëª… ìˆœìœ¼ë¡œ í´ë°±
            _bad_titles = ("ì¿ íŒ¡ ìƒí’ˆ", "ì¸ê¸°ìƒí’ˆ", "", None)
            if not product.title or product.title in _bad_titles:
                # 1ì°¨ í´ë°±: ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” ë°°ë„ˆì½”ë“œ altì—ì„œ ì¶”ì¶œëœ ìƒí’ˆëª…
                if product_name:
                    app.logger.debug(f"[FALLBACK] Using product_name: {product_name}")
                    product = Product(
                        title=product_name,
                        description=f"{product_name} - ì¿ íŒ¡ ìµœì €ê°€ ìƒí’ˆ",
                        url=coupang_link,
                        affiliate_link=affiliate_link,
                        scraped_at=product.scraped_at,
                    )
                else:
                    app.logger.debug("[FALLBACK] WARNING: No product_name, using default")
            elif product_name and product.title != product_name:
                # ìŠ¤í¬ë˜í•‘ ì„±ê³µí–ˆì§€ë§Œ ë°°ë„ˆ altì™€ ë‹¤ë¥¸ ê²½ìš° â†’ ë°°ë„ˆ alt ìš°ì„  (ë” ì •í™•)
                print(f"[V2] ë°°ë„ˆì½”ë“œ ìƒí’ˆëª…ìœ¼ë¡œ êµì²´: {product.title} â†’ {product_name}")
                product = Product(
                    title=product_name,
                    description=product.description or f"{product_name} - ì¿ íŒ¡ ìµœì €ê°€",
                    url=coupang_link,
                    affiliate_link=affiliate_link,
                    scraped_at=product.scraped_at,
                    price=getattr(product, 'price', ''),
                    images=getattr(product, 'images', []),
                )

            # í•­ìƒ ìˆ˜ìµ ë§í¬ë¥¼ íŒŒíŠ¸ë„ˆìŠ¤ ë§í¬ë¡œ ì„¤ì •
            product.affiliate_link = affiliate_link

            product_info = {
                "title": product.title,
                "description": product.description or "",
                "price": product.price or "",
                "image_urls": product.image_urls[:3] if product.image_urls else [],
                "affiliate_link": affiliate_link,  # ìˆ˜ìµ ë§í¬ (link.coupang.com/a/...)
                "product_url": coupang_link,        # ìƒí’ˆì •ë³´ ë§í¬ (coupang.com/vp/products/...)
            }
            job["product_info"] = product_info

            job["events"].put({
                "type": "v2_step", "step": 2, "name": "link_analysis",
                "status": "complete",
                "detail": f"ìƒí’ˆ: {product.title}",
                "timestamp": datetime.now().isoformat(),
            })

            # Step 3: AI ì½˜í…ì¸  ì´ˆì•ˆ ìƒì„±
            job["events"].put({
                "type": "v2_step", "step": 3, "name": "ai_content",
                "status": "running", "detail": "ë¸”ë¡œê·¸ + ìˆí¼ ëŒ€ë³¸ AI ìƒì„± ì¤‘ (Gemini ë¬´ë£Œ)...",
                "timestamp": datetime.now().isoformat(),
            })

            try:
                from affiliate_system.ai_generator import AIGenerator
                generator = AIGenerator()
                # ë””ë²„ê·¸: AI ìƒì„± ì§ì „ ìµœì¢… product.title í™•ì¸
                app.logger.debug(f"[AI_GEN] FINAL product.title={product.title}")
                app.logger.debug(f"[AI_GEN] FINAL product.description={str(product.description)[:100]}")

                # V2 ë¸”ë¡œê·¸ ì½˜í…ì¸  â€” ìˆ˜ìµ ë§í¬ë¡œ ìƒì„±
                blog_content = generator.generate_blog_content_v2(product, affiliate_link)
                job["draft"] = {
                    "blog": blog_content,
                    "product": product_info,
                }

                # V2 ìˆí¼ í›„í‚¹ ëŒ€ë³¸
                try:
                    shorts_scenes = generator.generate_shorts_hooking_script(
                        product, persona="", coupang_link=affiliate_link, dm_keyword="ë§í¬"
                    )
                    # list[dict] â†’ {"scenes": [...]} í˜•íƒœë¡œ ê°ì‹¸ê¸° (Step 7 í˜¸í™˜)
                    if isinstance(shorts_scenes, list):
                        shorts_script = {"scenes": shorts_scenes}
                    elif isinstance(shorts_scenes, dict) and "scenes" in shorts_scenes:
                        shorts_script = shorts_scenes
                    else:
                        shorts_script = {"scenes": []}
                    job["shorts_script"] = shorts_script
                    job["draft"]["shorts"] = shorts_script
                    print(f"[V2] ìˆí¼ ëŒ€ë³¸: {len(shorts_script.get('scenes', []))}ì¥ë©´ ìƒì„± ì™„ë£Œ")
                except Exception as se:
                    import traceback
                    print(f"[V2] ìˆí¼ ëŒ€ë³¸ ìƒì„± ì‹¤íŒ¨: {se}")
                    traceback.print_exc()
                    job["draft"]["shorts"] = {"error": str(se)}

                job["events"].put({
                    "type": "v2_step", "step": 3, "name": "ai_content",
                    "status": "complete",
                    "detail": f"ë¸”ë¡œê·¸ {len(blog_content.get('body_sections', []))}ì„¹ì…˜ + ìˆí¼ ëŒ€ë³¸ ìƒì„± ì™„ë£Œ",
                    "timestamp": datetime.now().isoformat(),
                })

            except Exception as ai_err:
                job["events"].put({
                    "type": "v2_step", "step": 3, "name": "ai_content",
                    "status": "error", "detail": str(ai_err),
                    "timestamp": datetime.now().isoformat(),
                })

            # í™•ì¸ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜
            job["state"] = V2PipelineState.AWAITING_CONFIRM
            job["events"].put({
                "type": "state_change",
                "state": V2PipelineState.AWAITING_CONFIRM,
                "message": "âœ… ë¶„ì„ ì™„ë£Œ! ì´ˆì•ˆì„ í™•ì¸í•˜ê³  ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
                "draft": job["draft"],
                "timestamp": datetime.now().isoformat(),
            })

        except Exception as e:
            job["state"] = V2PipelineState.ERROR
            job["error"] = str(e)
            job["events"].put({
                "type": "error", "error": str(e),
                "timestamp": datetime.now().isoformat(),
            })

    thread = threading.Thread(target=analyze, daemon=True)
    thread.start()

    return jsonify({"job_id": job_id, "state": V2PipelineState.ANALYZING})


@app.route('/api/v2/campaign/<job_id>/confirm', methods=['POST'])
def v2_confirm_execute(job_id):
    """V2 ì‹¤í–‰ í™•ì¸ â†’ ë‚˜ë¨¸ì§€ 7ë‹¨ê³„ (ë¯¸ë””ì–´ í¬ë¡¤ë§ ~ Drive ì•„ì¹´ì´ë¹™) ì‹¤í–‰."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    if job["state"] != V2PipelineState.AWAITING_CONFIRM:
        return jsonify({"error": f"í˜„ì¬ ìƒíƒœ: {job['state']}, ì‹¤í–‰ í™•ì¸ ë¶ˆê°€"}), 400

    # í”Œë«í¼ë³„ ì—…ë¡œë“œ í† ê¸€ ì €ì¥
    confirm_data = request.json or {}
    job["upload_youtube"] = confirm_data.get("upload_youtube", False)
    job["upload_instagram"] = confirm_data.get("upload_instagram", False)
    job["upload_naver"] = confirm_data.get("upload_naver", False)

    job["state"] = V2PipelineState.EXECUTING
    job["events"].put({
        "type": "state_change",
        "state": V2PipelineState.EXECUTING,
        "message": "ğŸš€ ì‹¤í–‰ ì‹œì‘! 10ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§„í–‰ ì¤‘...",
        "timestamp": datetime.now().isoformat(),
    })

    def execute():
        try:
            coupang_link = job["coupang_link"]
            affiliate_link = job.get("affiliate_link", coupang_link)  # ìˆ˜ìµ ë§í¬
            banner_tag = job.get("banner_tag", "")  # ì¿ íŒ¡ ë°°ë„ˆ ì½”ë“œ
            draft = job.get("draft", {})
            blog_content = draft.get("blog", {})
            product_info = job.get("product_info", {})
            product_title = product_info.get("title", "ìƒí’ˆ")

            # Step 4: ìŠ¤ë§ˆíŠ¸ ë¯¸ë””ì–´ í¬ë¡¤ë§ + AI ì´ë¯¸ì§€ ìƒì„±
            job["events"].put({
                "type": "v2_step", "step": 4, "name": "media_crawl",
                "status": "running", "detail": "Gemini í‚¤ì›Œë“œ ë¶„ì„ + ë¯¸ë””ì–´ ìˆ˜ì§‘ + AI ì´ë¯¸ì§€ ìƒì„±...",
                "timestamp": datetime.now().isoformat(),
            })
            blog_images = []
            video_sources = []
            ai_images = []
            try:
                from affiliate_system.media_collector import OmniMediaCollector, MediaCollector
                from affiliate_system.ai_generator import AIGenerator
                omni = OmniMediaCollector()
                gen = AIGenerator()

                # â”€â”€ Gemini SmartMediaMatcher: ì£¼ì œ ë¶„ì„ â†’ ìµœì  í‚¤ì›Œë“œ ìƒì„± â”€â”€
                product_features = product_info.get("features", "")
                if isinstance(product_features, list):
                    product_features = ", ".join(product_features)
                category = product_info.get("category", "")
                smart_keywords = gen.generate_smart_media_keywords(
                    product_name=product_title,
                    category=category,
                    product_features=product_features,
                )
                job["smart_keywords"] = smart_keywords
                job["category"] = smart_keywords.get("category_detected", category)
                job["product_name"] = product_title

                # ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
                image_kw_en = smart_keywords.get("image_keywords_en", [product_title])
                image_kw_ko = smart_keywords.get("image_keywords_ko", [product_title])
                all_image_kw = image_kw_en + image_kw_ko
                product_image_urls = product_info.get("image_urls", [])

                blog_images = omni.collect_blog_images(
                    product_title=product_title,
                    image_keywords=all_image_kw[:7],
                    product_image_urls=product_image_urls,
                    count=5,
                )

                # ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œë¡œ ë¹„ë””ì˜¤ ê²€ìƒ‰
                video_kw_en = smart_keywords.get("video_keywords_en", [])
                search_en = video_kw_en[0] if video_kw_en else gen.translate_for_search(product_title)
                try:
                    video_sources = omni.collect_video_sources(
                        product_title=product_title,
                        search_keyword_en=search_en,
                        count=6,
                    )
                except Exception:
                    video_sources = []

                # â”€â”€ Gemini Imagen 4.0: AI ì´ë¯¸ì§€ ìƒì„± (ë¶€ì¡±ë¶„ ë³´ì¶© + ê³ í€„ CTA) â”€â”€
                ai_prompts = smart_keywords.get("ai_image_prompts", [])
                if ai_prompts:
                    try:
                        from affiliate_system.config import V2_BLOG_DIR
                        ai_images = gen.generate_ai_images(
                            prompts=ai_prompts[:3],
                            output_dir=str(V2_BLOG_DIR / "ai_generated"),
                            count_per_prompt=1,
                            aspect_ratio="9:16",
                        )
                        # AI ì´ë¯¸ì§€ë¥¼ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ í’€ì— ì¶”ê°€
                        blog_images.extend(ai_images)
                    except Exception as ai_err:
                        print(f"[V2] AI ì´ë¯¸ì§€ ìƒì„± ìŠ¤í‚µ: {ai_err}")

                job["events"].put({
                    "type": "v2_step", "step": 4, "name": "media_crawl",
                    "status": "complete",
                    "detail": (
                        f"í¬ë¡¤ë§ ì´ë¯¸ì§€ {len(blog_images)-len(ai_images)}ì¥ + "
                        f"AI ì´ë¯¸ì§€ {len(ai_images)}ì¥ + "
                        f"ì˜ìƒ {len(video_sources)}ê°œ ìˆ˜ì§‘"
                    ),
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as me:
                import traceback
                print(f"[V2] Step 4 ë¯¸ë””ì–´ í¬ë¡¤ë§ ì—ëŸ¬: {me}")
                print(traceback.format_exc())
                job["events"].put({
                    "type": "v2_step", "step": 4, "name": "media_crawl",
                    "status": "error", "detail": str(me),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 5: ë¸”ë¡œê·¸ HTML ì¡°ë¦½
            job["events"].put({
                "type": "v2_step", "step": 5, "name": "blog_compose",
                "status": "running", "detail": "ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ êµì°¨ ë°°ì¹˜ HTML ìƒì„± ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            blog_html = ""
            try:
                from affiliate_system.blog_html_generator import NaverBlogHTMLGenerator
                html_gen = NaverBlogHTMLGenerator()
                blog_html = html_gen.generate_blog_html(
                    title=blog_content.get("title", product_info.get("title", "")),
                    intro=blog_content.get("intro", ""),
                    body_sections=blog_content.get("body_sections", []),
                    image_paths=[p for p in blog_images if p],  # ë¹ˆ ê²½ë¡œ í•„í„°ë§
                    coupang_link=affiliate_link,  # ìˆ˜ìµ ë§í¬ ì‚¬ìš©!
                    hashtags=blog_content.get("hashtags", []),
                    banner_tag=banner_tag,  # ì¿ íŒ¡ ë°°ë„ˆ ì½”ë“œ
                )
                job["blog_html"] = blog_html
                job["events"].put({
                    "type": "v2_step", "step": 5, "name": "blog_compose",
                    "status": "complete",
                    "detail": f"HTML {len(blog_html)}ì ìƒì„± (ì´ë¯¸ì§€ {len(blog_images)}ì¥ êµì°¨ ë°°ì¹˜)",
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as he:
                job["events"].put({
                    "type": "v2_step", "step": 5, "name": "blog_compose",
                    "status": "error", "detail": str(he),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 6: ì˜ìƒ ì„¸íƒ
            job["events"].put({
                "type": "v2_step", "step": 6, "name": "video_launder",
                "status": "running", "detail": "4ë‹¨ê³„ FFmpeg GPU ì„¸íƒ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            laundered_videos = []
            try:
                if video_sources:
                    from affiliate_system.video_launderer import VideoLaunderer
                    launderer = VideoLaunderer()
                    video_paths = [v["path"] for v in video_sources if v.get("path")]
                    laundered_videos = launderer.batch_launder(video_paths)
                    job["events"].put({
                        "type": "v2_step", "step": 6, "name": "video_launder",
                        "status": "complete",
                        "detail": f"{len(laundered_videos)}ê°œ ì˜ìƒ ì„¸íƒ ì™„ë£Œ",
                        "timestamp": datetime.now().isoformat(),
                    })
                else:
                    # ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì—†ìŒ â†’ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ë¥¼ ì˜ìƒ í´ë¦½ìœ¼ë¡œ ë³€í™˜ (Ken Burns)
                    if blog_images:
                        import subprocess
                        from affiliate_system.config import V2_SHORTS_DIR, FFMPEG_CRF
                        _img_vid_dir = Path(V2_SHORTS_DIR) / "img_clips"
                        _img_vid_dir.mkdir(parents=True, exist_ok=True)

                        for img_i, img_path in enumerate(blog_images[:6]):
                            try:
                                out_clip = str(_img_vid_dir / f"img_clip_{img_i}_{job_id[:8]}.mp4")
                                # FFmpeg: ì´ë¯¸ì§€ â†’ 8ì´ˆ ì˜ìƒ (zoompan Ken Burns íš¨ê³¼)
                                subprocess.run([
                                    "ffmpeg", "-y",
                                    "-loop", "1", "-i", str(img_path),
                                    "-vf", (
                                        "scale=1080:1920:force_original_aspect_ratio=increase,"
                                        "crop=1080:1920,"
                                        "zoompan=z='min(zoom+0.0015,1.3)':d=240:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':s=1080x1920:fps=30"
                                    ),
                                    "-t", "8",
                                    "-c:v", "libx264",
                                    "-crf", FFMPEG_CRF,
                                    "-preset", "medium",
                                    "-pix_fmt", "yuv420p",
                                    "-an",  # ì˜¤ë””ì˜¤ ì—†ìŒ
                                    out_clip,
                                ], capture_output=True, timeout=60)

                                if os.path.exists(out_clip) and os.path.getsize(out_clip) > 10000:
                                    laundered_videos.append(out_clip)
                            except Exception as _img_err:
                                print(f"[V2] ì´ë¯¸ì§€â†’ì˜ìƒ ë³€í™˜ ì‹¤íŒ¨ [{img_i}]: {_img_err}")

                        print(f"[V2] ì´ë¯¸ì§€â†’ì˜ìƒ í´ë°±: {len(laundered_videos)}ê°œ ìƒì„±")

                    job["events"].put({
                        "type": "v2_step", "step": 6, "name": "video_launder",
                        "status": "complete",
                        "detail": f"ì´ë¯¸ì§€â†’ì˜ìƒ í´ë°±: {len(laundered_videos)}ê°œ í´ë¦½ ìƒì„±" if laundered_videos else "ì˜ìƒ/ì´ë¯¸ì§€ ì—†ìŒ",
                        "timestamp": datetime.now().isoformat(),
                    })
            except Exception as le:
                job["events"].put({
                    "type": "v2_step", "step": 6, "name": "video_launder",
                    "status": "error", "detail": str(le),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 7: ìˆí¼ ë Œë”ë§
            job["events"].put({
                "type": "v2_step", "step": 7, "name": "shorts_render",
                "status": "running", "detail": "TTS + ìë§‰ ì‹±í¬ + ìˆí¼ ì¡°ë¦½ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            shorts_path = None
            try:
                _dbg = f"Step 7 ì²´í¬: lv={len(laundered_videos) if laundered_videos else 0}, script={type(job.get('shorts_script'))}, has_script={bool(job.get('shorts_script'))}"
                app.logger.debug(_dbg)
                job["results"]["step7_debug"] = _dbg
                if laundered_videos and job.get("shorts_script"):
                    from affiliate_system.video_launderer import (
                        EmotionTTSEngine, SubtitleGenerator, ShortsRenderer,
                        ProShortsRenderer, _detect_bgm_genre,
                    )
                    from affiliate_system.config import V2_TTS_DIR, V2_SUBTITLE_DIR, V2_SHORTS_DIR

                    script = job["shorts_script"]
                    # list ë˜ëŠ” {"scenes": [...]} ë‘˜ ë‹¤ ì§€ì›
                    if isinstance(script, list):
                        scenes_data = script
                    elif isinstance(script, dict):
                        scenes_data = script.get("scenes", [])
                    else:
                        scenes_data = []

                    app.logger.debug(f"Step7 ì§„ì…: scenes={len(scenes_data)}, lv={len(laundered_videos)}")

                    # emotion ìœ íš¨ì„± ê²€ì¦
                    valid_emotions = {"excited", "friendly", "urgent", "dramatic", "calm", "hyped"}
                    for sd in scenes_data:
                        emo = sd.get("emotion", "friendly")
                        if emo not in valid_emotions:
                            sd["emotion"] = "friendly"

                    # TTS ìƒì„±
                    app.logger.debug("TTS ì‹œì‘...")
                    tts_engine = EmotionTTSEngine()
                    scenes = tts_engine.generate_scenes_tts(scenes_data, job_id)
                    app.logger.debug(f"TTS ì™„ë£Œ: {len(scenes)}ì¥ë©´")

                    # ìë§‰ ìƒì„±
                    sub_gen = SubtitleGenerator()
                    subtitle_path = sub_gen.generate_ass_from_scenes(scenes, job_id)
                    if not subtitle_path:
                        subtitle_path = str(V2_SUBTITLE_DIR / f"{job_id}_subtitle.ass")
                    app.logger.debug(f"ìë§‰: {subtitle_path}")

                    # ì„¸íƒëœ ì˜ìƒì„ sceneì— ë§¤í•‘ (round-robin ìˆœí™˜)
                    render_scenes = []
                    for i, sc in enumerate(scenes):
                        video_idx = i % len(laundered_videos)
                        video_path = laundered_videos[video_idx]
                        render_scenes.append({
                            "video_clip_path": video_path,
                            "tts_path": sc.get("tts_path", "") or "",
                            "tts_duration": sc.get("tts_duration", sc.get("duration", 3.0)),
                            "text": sc.get("text", ""),
                            "emotion": sc.get("emotion", "friendly"),
                        })

                    # ìµœì¢… ë Œë”ë§ â€” ProShortsRenderer V3 (ëª¨ì…˜+ì „í™˜+BGM+ì»¬ëŸ¬ê·¸ë ˆì´ë”©)
                    app.logger.debug(f"ProShortsRenderer ì‹œì‘: {len(render_scenes)}ì¥ë©´")
                    product_name = job.get("product_name", job.get("product_info", {}).get("title", "unknown"))
                    category = job.get("category", "")
                    try:
                        renderer = ProShortsRenderer()
                        result_path = renderer.render_pro_shorts(
                            scenes=render_scenes,
                            campaign_id=job_id,
                            subtitle_path=subtitle_path,
                            product_name=product_name,
                            category=category,
                        )
                    except Exception as pro_err:
                        app.logger.debug(f"ProShortsRenderer ì‹¤íŒ¨, í´ë°±: {pro_err}")
                        renderer = ShortsRenderer()
                        result_path = renderer.render_final_shorts(
                            scenes=render_scenes,
                            campaign_id=job_id,
                            subtitle_path=subtitle_path,
                            coupang_link=affiliate_link,
                        )
                    app.logger.debug(f"ë Œë”ë§ ê²°ê³¼: {result_path}")
                    if result_path:
                        shorts_path = result_path

                    job["events"].put({
                        "type": "v2_step", "step": 7, "name": "shorts_render",
                        "status": "complete",
                        "detail": f"ìˆí¼ ë Œë”ë§ ì™„ë£Œ: {Path(shorts_path).name}" if shorts_path else "ë Œë”ë§ ì‹¤íŒ¨",
                        "timestamp": datetime.now().isoformat(),
                    })
                else:
                    skip_reason = f"laundered={len(laundered_videos) if laundered_videos else 0}, script={bool(job.get('shorts_script'))}"
                    job["results"]["shorts_skip"] = skip_reason
                    job["events"].put({
                        "type": "v2_step", "step": 7, "name": "shorts_render",
                        "status": "complete", "detail": f"ìˆí¼ ìŠ¤í‚µ: {skip_reason}",
                        "timestamp": datetime.now().isoformat(),
                    })
            except Exception as render_err:
                import traceback
                err_detail = traceback.format_exc()
                print(f"[V2] Step 7 ìˆí¼ ë Œë”ë§ ì—ëŸ¬: {render_err}")
                print(err_detail)
                job["results"]["shorts_error"] = f"{render_err}\n{err_detail}"
                job["events"].put({
                    "type": "v2_step", "step": 7, "name": "shorts_render",
                    "status": "error", "detail": str(render_err),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 8: ì¸ë„¤ì¼
            job["events"].put({
                "type": "v2_step", "step": 8, "name": "thumbnail",
                "status": "running", "detail": "í”Œë«í¼ë³„ ì¸ë„¤ì¼ ìƒì„± ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            try:
                # ì¸ë„¤ì¼ì€ V1 íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
                job["events"].put({
                    "type": "v2_step", "step": 8, "name": "thumbnail",
                    "status": "complete", "detail": "ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ (ë˜ëŠ” ìƒëµ)",
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as te:
                job["events"].put({
                    "type": "v2_step", "step": 8, "name": "thumbnail",
                    "status": "error", "detail": str(te),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 9: í”Œë«í¼ë³„ ìë™ ì—…ë¡œë“œ (ON/OFF ìŠ¤ìœ„ì¹˜ ê¸°ë°˜)
            upload_youtube = job.get("upload_youtube", False)
            upload_instagram = job.get("upload_instagram", False)
            upload_naver = job.get("upload_naver", False)
            any_upload = upload_youtube or upload_instagram or upload_naver

            job["events"].put({
                "type": "v2_step", "step": 9, "name": "upload_ready",
                "status": "running",
                "detail": f"ì—…ë¡œë“œ: YT={'ON' if upload_youtube else 'OFF'} | IG={'ON' if upload_instagram else 'OFF'} | Blog={'ON' if upload_naver else 'OFF'}",
                "timestamp": datetime.now().isoformat(),
            })
            upload_results = {}
            try:
                job["results"]["blog_html"] = blog_html
                job["results"]["blog_images"] = blog_images
                job["results"]["shorts_path"] = shorts_path
                job["results"]["laundered_videos"] = laundered_videos

                # í”Œë«í¼ë³„ ìë™ ì—…ë¡œë“œ ì‹¤í–‰
                if any_upload:
                    try:
                        from affiliate_system.auto_uploader import StealthUploader
                        uploader = StealthUploader()
                        uploaded = []

                        if upload_youtube and shorts_path:
                            try:
                                if uploader.youtube_auth():
                                    yt_result = uploader.youtube_upload_v2(
                                        video_path=shorts_path,
                                        title=f"{product_title} ì¶”ì²œ #Shorts",
                                        description=f"#{product_title} #ì¿ íŒ¡ #ì¶”ì²œ #ì‡¼ì¸ ",
                                    )
                                    if yt_result:
                                        uploaded.append("YouTube")
                                        upload_results["youtube"] = yt_result
                            except Exception as yt_err:
                                upload_results["youtube_error"] = str(yt_err)

                        if upload_instagram and shorts_path:
                            try:
                                if uploader.instagram_auth():
                                    ig_result = uploader.instagram_upload_reel_v2(
                                        video_path=shorts_path,
                                        caption=f"{product_title} ì†”ì§ ì¶”ì²œ! ğŸ’¯\n#ì¿ íŒ¡ #{product_title.replace(' ', '')} #ì¶”ì²œ",
                                    )
                                    if ig_result:
                                        uploaded.append("Instagram")
                                        upload_results["instagram"] = ig_result
                            except Exception as ig_err:
                                upload_results["instagram_error"] = str(ig_err)

                        if upload_naver and blog_html:
                            try:
                                naver_result = uploader.naver_blog_post_v2(
                                    html_content=blog_html,
                                    title=product_title,
                                )
                                if naver_result:
                                    uploaded.append("Naver")
                                    upload_results["naver"] = naver_result
                            except Exception as nv_err:
                                upload_results["naver_error"] = str(nv_err)

                        upload_detail = f"ì—…ë¡œë“œ ì™„ë£Œ: {', '.join(uploaded)}" if uploaded else "ì—…ë¡œë“œ ëŒ€ìƒ ì—†ìŒ"
                    except Exception as up_err:
                        upload_detail = f"ì—…ë¡œë” ë¡œë“œ ì‹¤íŒ¨: {up_err}"
                else:
                    upload_detail = "ìë™ ì—…ë¡œë“œ OFF â€” ê²°ê³¼ë¬¼ í™•ì¸ í›„ ìˆ˜ë™ ì—…ë¡œë“œ"

                job["results"]["upload_results"] = upload_results

                job["events"].put({
                    "type": "v2_step", "step": 9, "name": "upload_ready",
                    "status": "complete",
                    "detail": upload_detail,
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as ue:
                job["events"].put({
                    "type": "v2_step", "step": 9, "name": "upload_ready",
                    "status": "error", "detail": str(ue),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 10: Drive ì•„ì¹´ì´ë¹™
            job["events"].put({
                "type": "v2_step", "step": 10, "name": "drive_archive",
                "status": "running", "detail": "Google Drive ì•„ì¹´ì´ë¹™ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            try:
                from affiliate_system.drive_manager import DriveArchiver
                archiver = DriveArchiver()
                if archiver.authenticate():
                    # V2 í”Œë«í¼ë³„ íŒŒì¼ ë¶„ë¥˜ â€” ë°”ë¡œ í´ë¦­í•´ì„œ ë³¼ ìˆ˜ ìˆëŠ” êµ¬ì¡°
                    valid_images = [p for p in blog_images if p and Path(p).exists()]
                    drive_files = {
                        # ë„¤ì´ë²„ë¸”ë¡œê·¸: ë¸”ë¡œê·¸ HTML + ì´ë¯¸ì§€
                        "naver_blog": [],
                        # ì¸ìŠ¤íƒ€ê·¸ë¨ìˆì¸ : ìˆí¼ ì˜ìƒ (ë™ì¼ ì˜ìƒ ê³µìœ )
                        "instagram_shorts": [],
                        # ìœ íŠœë¸Œìˆì¸ : ìˆí¼ ì˜ìƒ (ë™ì¼ ì˜ìƒ ê³µìœ )
                        "youtube_shorts": [],
                    }

                    # ë„¤ì´ë²„ë¸”ë¡œê·¸ í´ë”: HTML + ì´ë¯¸ì§€
                    if blog_html:
                        blog_html_path = Path(WORK_DIR) / f"blog_{job_id}.html"
                        blog_html_path.write_text(blog_html, encoding="utf-8")
                        drive_files["naver_blog"].append(str(blog_html_path))
                    drive_files["naver_blog"].extend(valid_images)

                    # ìˆí¼ ì˜ìƒ â†’ ì¸ìŠ¤íƒ€ê·¸ë¨ + ìœ íŠœë¸Œ ì–‘ìª½ì— ì—…ë¡œë“œ
                    if shorts_path and Path(shorts_path).exists():
                        drive_files["instagram_shorts"].append(shorts_path)
                        drive_files["youtube_shorts"].append(shorts_path)

                    # ì„¸íƒëœ ì›ë³¸ ì˜ìƒë„ ìœ íŠœë¸Œ í´ë”ì— ì¶”ê°€ (í¸ì§‘ìš© ì†ŒìŠ¤)
                    for lv in laundered_videos:
                        if lv and Path(lv).exists():
                            drive_files["youtube_shorts"].append(lv)

                    # ì„ì‹œ Campaign ê°ì²´ ìƒì„± â€” ì¬ìŠ¤í¬ë˜í•‘ ì•Šê³  ì €ì¥ëœ ì •ë³´ ì‚¬ìš©
                    from affiliate_system.models import Campaign, AIContent, CampaignStatus, Product
                    temp_product = Product(
                        title=product_title,
                        description=product_info.get("description", ""),
                        url=coupang_link,
                        affiliate_link=affiliate_link,
                    )
                    temp_campaign = Campaign(
                        id=job_id, product=temp_product,
                        ai_content=AIContent(platform_contents={}),
                        status=CampaignStatus.COMPLETE,
                        target_platforms=[],
                        platform_videos={}, platform_thumbnails={},
                        created_at=datetime.now(),
                    )
                    archive_result = archiver.archive_campaign(
                        temp_campaign, drive_files, v2=True
                    )
                    if archive_result["ok"]:
                        job["results"]["drive_url"] = archive_result.get("folder_url", "")
                        job["results"]["drive_platforms"] = archive_result.get("platform_urls", {})
                        job["events"].put({
                            "type": "v2_step", "step": 10, "name": "drive_archive",
                            "status": "complete",
                            "detail": f"Drive ì•„ì¹´ì´ë¹™ ì™„ë£Œ: {archive_result['files_uploaded']}ê°œ íŒŒì¼ (3 í”Œë«í¼)",
                            "timestamp": datetime.now().isoformat(),
                        })
                    else:
                        job["events"].put({
                            "type": "v2_step", "step": 10, "name": "drive_archive",
                            "status": "error", "detail": "Drive ì—…ë¡œë“œ ì¼ë¶€ ì‹¤íŒ¨",
                            "timestamp": datetime.now().isoformat(),
                        })
                else:
                    job["events"].put({
                        "type": "v2_step", "step": 10, "name": "drive_archive",
                        "status": "error", "detail": "Drive ì¸ì¦ ì‹¤íŒ¨",
                        "timestamp": datetime.now().isoformat(),
                    })
            except Exception as de:
                job["events"].put({
                    "type": "v2_step", "step": 10, "name": "drive_archive",
                    "status": "error", "detail": str(de),
                    "timestamp": datetime.now().isoformat(),
                })

            # ì™„ë£Œ
            job["state"] = V2PipelineState.COMPLETE
            job["events"].put({
                "type": "v2_complete",
                "message": "ğŸ‰ V2 íŒŒì´í”„ë¼ì¸ 10ë‹¨ê³„ ì™„ë£Œ!",
                "results": _safe_serialize(job["results"]),
                "timestamp": datetime.now().isoformat(),
            })

            # ìº í˜ì¸ DB ì €ì¥
            _save_campaign(
                job_id, product_info.get("title", "V2 Campaign"),
                "V2", job["platforms"], "complete",
                results=_safe_serialize(job["results"]),
            )

        except Exception as e:
            job["state"] = V2PipelineState.ERROR
            job["error"] = str(e)
            job["events"].put({
                "type": "error", "error": str(e),
                "timestamp": datetime.now().isoformat(),
            })

    thread = threading.Thread(target=execute, daemon=True)
    thread.start()

    return jsonify({"job_id": job_id, "state": V2PipelineState.EXECUTING})


@app.route('/api/v2/campaign/<job_id>/status')
def v2_campaign_status(job_id):
    """V2 ìº í˜ì¸ ìƒíƒœ ì¡°íšŒ."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404

    return jsonify({
        "job_id": job_id,
        "state": job["state"],
        "coupang_link": job.get("coupang_link"),
        "product_info": job.get("product_info"),
        "draft": job.get("draft"),
        "blog_html": job.get("blog_html", ""),
        "results": _safe_serialize(job.get("results", {})),
        "error": job.get("error"),
        "created_at": job.get("created_at"),
    })


@app.route('/api/v2/campaign/<job_id>/stream')
def v2_stream(job_id):
    """V2 SSE ìŠ¤íŠ¸ë¦¬ë° â€” 10ë‹¨ê³„ ì§„í–‰ìƒí™©."""
    def generate():
        job = v2_jobs.get(job_id)
        if not job:
            yield f"data: {json.dumps({'type': 'error', 'error': 'Job not found'})}\n\n"
            return

        q = job["events"]
        while job["state"] not in (V2PipelineState.COMPLETE, V2PipelineState.ERROR):
            while True:
                try:
                    event = q.get_nowait()
                except Exception:
                    break
                yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"
            time.sleep(0.3)

        # ì”ì—¬ ì´ë²¤íŠ¸ flush
        while True:
            try:
                event = q.get_nowait()
            except Exception:
                break
            yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"

        # ìµœì¢… ìƒíƒœ
        if job["state"] == V2PipelineState.COMPLETE:
            yield f"data: {json.dumps({'type': 'v2_done', 'results': _safe_serialize(job.get('results', {}))}, ensure_ascii=False, default=str)}\n\n"
        elif job["state"] == V2PipelineState.ERROR:
            yield f"data: {json.dumps({'type': 'error', 'error': job.get('error', 'Unknown error')})}\n\n"

    return Response(generate(), mimetype='text/event-stream',
                    headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})


@app.route('/api/v2/campaign/<job_id>/blog-preview')
def v2_blog_preview(job_id):
    """V2 ë¸”ë¡œê·¸ HTML ë¯¸ë¦¬ë³´ê¸°."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404

    blog_html = job.get("blog_html", "")
    if blog_html:
        return Response(blog_html, mimetype='text/html; charset=utf-8')
    return jsonify({"error": "ë¸”ë¡œê·¸ HTML ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ"}), 404


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# V3 â€” ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ìˆ˜ìµ ê·¹ëŒ€í™” í†µí•© íŒŒì´í”„ë¼ì¸ (8ë‹¨ê³„)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

v3_jobs = {}  # job_id -> {state, ...}

class V3PipelineState:
    IDLE = "idle"
    ANALYZING = "analyzing"
    AWAITING_CONFIRM = "awaiting_confirm"
    EXECUTING = "executing"
    COMPLETE = "complete"
    ERROR = "error"

V3_STEPS = [
    {"step": 1, "name": "analyze",    "label": "ì…ë ¥ ë¶„ì„",         "category": "ì¤€ë¹„",     "module": "coupang_scraper"},
    {"step": 2, "name": "content",    "label": "AI ì½˜í…ì¸  ìƒì„±",   "category": "ì¤€ë¹„",     "module": "ai_generator"},
    {"step": 3, "name": "collect",    "label": "ë¯¸ë””ì–´ ìˆ˜ì§‘",       "category": "ìˆ˜ì§‘",     "module": "OmniMediaCollector + yt-dlp"},
    {"step": 4, "name": "ai_media",   "label": "AI ë¯¸ë””ì–´ ìƒì„±",   "category": "AIìƒì„±",   "module": "NanoBanana + Imagen + VEO"},
    {"step": 5, "name": "naver",      "label": "ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìµœì í™”", "category": "ìµœì í™”", "module": "blog_html_generator"},
    {"step": 6, "name": "youtube",    "label": "ìœ íŠœë¸Œ ì‡¼ì¸  ìµœì í™”",  "category": "ìµœì í™”", "module": "ProShortsRenderer (60fps)"},
    {"step": 7, "name": "instagram",  "label": "ì¸ìŠ¤íƒ€ ë¦´ìŠ¤ ìµœì í™”",  "category": "ìµœì í™”", "module": "ProShortsRenderer (30fps)"},
    {"step": 8, "name": "deploy",     "label": "ì—…ë¡œë“œ & ì•„ì¹´ì´ë¹™",   "category": "ë°°í¬",   "module": "StealthUploader + DriveArchiver"},
]

# â”€â”€ ë§ˆì´í¬ë¡œê¸‰ ë””í…Œì¼ í”„ë¡¬í”„íŠ¸ â”€â”€
V3_MICRO_DETAIL_PROMPT = """
AI Image Prompt Rules (V3 Micro-Detail):

[CAMERA & LENS]
- 85mm f/1.4: subject isolation, creamy bokeh, natural compression
- 50mm f/1.8: lifestyle, human field of view, zero distortion
- 35mm f/2.0: environmental context, wide product scene
- 135mm f/2.0: extreme background compression, dramatic separation
- 100mm macro: product texture at 0.5cm detail level
- Anamorphic 1.33x: horizontal lens flare, cinematic feel
- Tilt-shift: miniature effect, selective focus plane

[LIGHTING]
- Three-point studio: Key 45deg + Fill opposite + Back rim
- Rembrandt: triangle shadow on cheek, dramatic
- Golden hour: warm side light, skin tone optimization
- Window light + sheer curtain diffusion
- Rim/hair light for subject edge separation
- Volumetric haze: light particles in air, cinematic depth

[PERSON MICRO-DETAIL]
- Skin: natural texture, visible pores, peach fuzz, subtle glow, NO airbrushing
- Eyes: iris fiber pattern, limbal ring, 2-point catchlight, tear film shine
- Hair: individual strand separation, natural highlight gradient, flyaway strands
- Expression: Duchenne smile (genuine, eye crinkle), micro-expressions
- Hands: natural finger pose holding product, nail and knuckle detail
- Clothing: fabric texture (cotton weave, silk sheen, denim grain), wrinkle shadows
- Model: Korean/Asian, age-appropriate for product target demographic
- Pose: candid unstaged look, actual product usage scene

[COMPOSITION & POST]
- Rule of thirds, leading lines, negative space for text overlay
- Multi-layer depth: foreground blur + sharp subject + background bokeh
- Film grain: Kodak Portra 400 warm / Fuji Superia vivid / Cinestill 800T cinematic
- Color grading: warm(lifestyle) / cool(tech) / vibrant(food) / muted(fashion)
- 8K resolution, photorealistic, masterpiece quality
"""

# ì¹´ë©”ë¼/ì¡°ë„/ì¸ë¬¼ í”„ë¦¬ì…‹ í’€ (ëœë¤ ì¡°í•©ìš©)
_V3_CAMERAS = [
    "85mm f/1.4 lens, shallow depth of field, creamy bokeh",
    "50mm f/1.8 lens, natural perspective, zero distortion",
    "35mm f/2.0 wide angle, environmental context",
    "135mm f/2.0 telephoto, dramatic background compression",
    "100mm macro lens, extreme detail close-up",
]
_V3_LIGHTINGS = [
    "three-point studio lighting, Rembrandt shadow",
    "golden hour warm rim lighting, skin tone optimized",
    "natural window light with sheer curtain diffusion",
    "cinematic volumetric haze lighting",
    "split lighting with neon accent, modern aesthetic",
]
_V3_PERSON_DETAILS = [
    "Korean model with natural skin texture, visible pores, genuine Duchenne smile, detailed iris with catchlight reflection",
    "Asian model in candid lifestyle pose, individual hair strands visible, natural hand pose holding product, clothing fabric texture",
    "Korean person with relaxed micro-expression, peach fuzz on skin, tear film shine in eyes, subtle blush on cheeks",
]
_V3_COMPOSITIONS = [
    "rule of thirds composition, multi-layered depth, Kodak Portra 400 film grain",
    "leading lines composition, foreground bokeh, Fuji Superia vivid color grading",
    "centered subject with negative space, Cinestill 800T cinematic grain, warm tones",
]


class V3WebPipeline:
    """V3 ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ì „ë¬¸ 8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸."""

    def __init__(self, job_id, topic, coupang_url, affiliate_link, banner_tag,
                 product_name, ai_provider, review_mode, upload_flags,
                 social_urls, events_queue):
        self.job_id = job_id
        self.topic = topic
        self.coupang_url = coupang_url
        self.affiliate_link = affiliate_link
        self.banner_tag = banner_tag
        self.product_name = product_name
        self.ai_provider = ai_provider
        self.review_mode = review_mode
        self.upload_flags = upload_flags  # {youtube, instagram, naver, drive}
        self.social_urls = social_urls or []
        self._q = events_queue
        # ë‚´ë¶€ ìƒíƒœ
        self.product = None
        self.product_info = {}
        self.blog_content = {}
        self.shorts_script = {}
        self.smart_keywords = {}
        self.blog_images = []
        self.video_sources = []
        self.ai_images = []
        self.ai_videos = []
        self.blog_html = ""
        self.yt_shorts_path = None
        self.ig_reels_path = None
        self.results = {}

    def _emit(self, step, name, status, detail=""):
        self._q.put({
            "type": "v3_step", "step": step, "name": name,
            "status": status, "detail": detail,
            "timestamp": datetime.now().isoformat(),
        })

    def _enhance_ai_prompts(self, base_prompts):
        """SmartMediaMatcher í”„ë¡¬í”„íŠ¸ì— ë§ˆì´í¬ë¡œ ë””í…Œì¼ ì£¼ì…."""
        import random
        enhanced = []
        for i, prompt in enumerate(base_prompts):
            cam = _V3_CAMERAS[i % len(_V3_CAMERAS)]
            light = _V3_LIGHTINGS[i % len(_V3_LIGHTINGS)]
            comp = _V3_COMPOSITIONS[i % len(_V3_COMPOSITIONS)]
            # ì¸ë¬¼ í‚¤ì›Œë“œ ê°ì§€ì‹œ ì¸ë¬¼ ë””í…Œì¼ ì¶”ê°€
            person_kw = ["person", "model", "woman", "man", "using", "holding", "lifestyle", "hand"]
            has_person = any(kw in prompt.lower() for kw in person_kw)
            person = _V3_PERSON_DETAILS[i % len(_V3_PERSON_DETAILS)] if has_person else ""
            enhanced_prompt = (
                f"{prompt}, shot with {cam}, {light}, {comp}, "
                f"{'(' + person + '), ' if person else ''}"
                f"8K resolution, photorealistic, masterpiece quality"
            )
            enhanced.append(enhanced_prompt)
        return enhanced

    def _call_nano_banana(self, prompt, output_path, resolution="4K"):
        """NanoBanana Pro (gemini-3-pro-image-preview) ì´ë¯¸ì§€ ìƒì„±."""
        import subprocess as sp
        script = os.path.expanduser(
            "~/.openclaw/workspace/skills/nano-banana-pro/scripts/generate_image.py"
        )
        if not os.path.exists(script):
            print(f"[V3] NanoBanana ìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ: {script}")
            return None
        cmd = ["uv", "run", script, "--prompt", prompt, "--filename", output_path, "--resolution", resolution]
        try:
            result = sp.run(cmd, capture_output=True, timeout=180, text=True, encoding="utf-8", errors="replace")
            if result.returncode == 0 and os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
                return output_path
            else:
                print(f"[V3] NanoBanana ì‹¤íŒ¨: rc={result.returncode}, stderr={result.stderr[:200]}")
        except Exception as e:
            print(f"[V3] NanoBanana ì—ëŸ¬: {e}")
        return None

    def _call_veo(self, prompt, output_path):
        """VEO 3.1 ì˜ìƒ ìƒì„± (graceful fallback)."""
        try:
            from google import genai
            from google.genai import types
            client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))
            operation = client.models.generate_videos(
                model="veo-3.1-generate-preview",
                prompt=prompt,
                config=types.GenerateVideosConfig(
                    aspect_ratio="9:16",
                    number_of_videos=1,
                ),
            )
            # í´ë§ ëŒ€ê¸° (ìµœëŒ€ 5ë¶„)
            for _ in range(60):
                if operation.done:
                    break
                time.sleep(5)
                operation = client.operations.get(operation)
            if operation.done and operation.response and operation.response.generated_videos:
                video = operation.response.generated_videos[0]
                with open(output_path, "wb") as f:
                    f.write(video.video.video_bytes)
                if os.path.exists(output_path) and os.path.getsize(output_path) > 10000:
                    return output_path
        except Exception as e:
            print(f"[V3] VEO 3.1 ì‹¤íŒ¨ (fallback): {e}")
        return None

    def run(self):
        """8ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰."""
        try:
            self._step_1_analyze()
            self._step_2_content()
            # review_modeë©´ ì—¬ê¸°ì„œ ì¼ì‹œì •ì§€ (ì™¸ë¶€ì—ì„œ confirm í˜¸ì¶œ í›„ ì¬ê°œ)
            if self.review_mode:
                return "awaiting_confirm"
            return self._run_steps_3_to_8()
        except Exception as e:
            self._q.put({"type": "error", "error": str(e), "timestamp": datetime.now().isoformat()})
            raise

    def resume_after_confirm(self):
        """ê²€í†  í™•ì¸ í›„ 3~8ë‹¨ê³„ ì‹¤í–‰."""
        return self._run_steps_3_to_8()

    def _run_steps_3_to_8(self):
        self._step_3_collect()
        self._step_4_ai_generate()
        self._step_5_naver()
        self._step_6_youtube()
        self._step_7_instagram()
        self._step_8_deploy()
        return "complete"

    # â”€â”€ Step 1: ì…ë ¥ ë¶„ì„ â”€â”€
    def _step_1_analyze(self):
        self._emit(1, "analyze", "running", "ì¿ íŒ¡ ìƒí’ˆ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        from affiliate_system.pipeline import ContentPipeline
        from affiliate_system.models import Product
        pipeline = ContentPipeline()
        product = pipeline._prepare_product(self.coupang_url)

        # ìƒí’ˆëª… í´ë°±: ë°°ë„ˆ alt â†’ ì‚¬ìš©ì ì…ë ¥ â†’ ìŠ¤í¬ë˜í•‘ ê²°ê³¼
        import re as _re
        if not self.product_name and self.banner_tag:
            m = _re.search(r'alt=["\']([^"\']+)["\']', self.banner_tag)
            if m:
                self.product_name = m.group(1).strip()

        _bad = ("ì¿ íŒ¡ ìƒí’ˆ", "ì¸ê¸°ìƒí’ˆ", "", None)
        if not product.title or product.title in _bad:
            if self.product_name:
                product = Product(
                    title=self.product_name,
                    description=f"{self.product_name} - ì¿ íŒ¡ ìµœì €ê°€ ìƒí’ˆ",
                    url=self.coupang_url,
                    affiliate_link=self.affiliate_link,
                    scraped_at=product.scraped_at,
                )
        elif self.product_name and product.title != self.product_name:
            product = Product(
                title=self.product_name,
                description=product.description or f"{self.product_name} - ì¿ íŒ¡ ìµœì €ê°€",
                url=self.coupang_url, affiliate_link=self.affiliate_link,
                scraped_at=product.scraped_at,
                price=getattr(product, 'price', ''),
                images=getattr(product, 'images', []),
            )
        product.affiliate_link = self.affiliate_link
        self.product = product
        self.product_info = {
            "title": product.title,
            "description": product.description or "",
            "price": product.price or "",
            "image_urls": product.image_urls[:5] if product.image_urls else [],
            "affiliate_link": self.affiliate_link,
            "product_url": self.coupang_url,
        }
        self._emit(1, "analyze", "complete", f"ìƒí’ˆ: {product.title}")

    # â”€â”€ Step 2: AI ì½˜í…ì¸  ìƒì„± â”€â”€
    def _step_2_content(self):
        self._emit(2, "content", "running", "ë¸”ë¡œê·¸ ê¸€ + ìˆí¼ ëŒ€ë³¸ AI ìƒì„± ì¤‘ (Gemini ë¬´ë£Œ)...")
        from affiliate_system.ai_generator import AIGenerator
        gen = AIGenerator()

        # ë¸”ë¡œê·¸ V2
        self.blog_content = gen.generate_blog_content_v2(self.product, self.affiliate_link)

        # ìˆí¼ í›„í‚¹ ëŒ€ë³¸
        try:
            scenes = gen.generate_shorts_hooking_script(
                self.product, persona="", coupang_link=self.affiliate_link, dm_keyword="ë§í¬"
            )
            if isinstance(scenes, list):
                self.shorts_script = {"scenes": scenes}
            elif isinstance(scenes, dict) and "scenes" in scenes:
                self.shorts_script = scenes
            else:
                self.shorts_script = {"scenes": []}
        except Exception as e:
            print(f"[V3] ìˆí¼ ëŒ€ë³¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.shorts_script = {"scenes": []}

        detail = f"ë¸”ë¡œê·¸ {len(self.blog_content.get('body_sections', []))}ì„¹ì…˜ + ìˆí¼ {len(self.shorts_script.get('scenes', []))}ì¥ë©´"
        self._emit(2, "content", "complete", detail)

        # review_modeì´ë©´ draft ì´ë²¤íŠ¸ ë°œìƒ
        if self.review_mode:
            self._q.put({
                "type": "draft_ready",
                "draft": {
                    "blog": self.blog_content,
                    "shorts": self.shorts_script,
                    "product": self.product_info,
                },
                "timestamp": datetime.now().isoformat(),
            })

    # â”€â”€ Step 3: ë¯¸ë””ì–´ ìˆ˜ì§‘ (ëª¨ë“  í”Œë«í¼) â”€â”€
    def _step_3_collect(self):
        self._emit(3, "collect", "running", "ëª¨ë“  í”Œë«í¼ì—ì„œ ì´ë¯¸ì§€/ì˜ìƒ ìˆ˜ì§‘ ì¤‘...")
        from affiliate_system.media_collector import OmniMediaCollector, MediaCollector
        from affiliate_system.ai_generator import AIGenerator
        gen = AIGenerator()
        omni = OmniMediaCollector()

        # SmartMediaMatcher í‚¤ì›Œë“œ ìƒì„±
        features = self.product_info.get("features", "")
        if isinstance(features, list):
            features = ", ".join(features)
        self.smart_keywords = gen.generate_smart_media_keywords(
            product_name=self.product_info["title"],
            category=self.product_info.get("category", ""),
            product_features=features,
        )

        # ì´ë¯¸ì§€ ìˆ˜ì§‘ (Pexels + Pixabay + Unsplash + Google + Pinterest)
        image_kw = self.smart_keywords.get("image_keywords_en", []) + self.smart_keywords.get("image_keywords_ko", [])
        product_images = self.product_info.get("image_urls", [])
        try:
            self.blog_images = omni.collect_blog_images(
                product_title=self.product_info["title"],
                image_keywords=image_kw[:7],
                product_image_urls=product_images,
                count=7,
            )
        except Exception as e:
            print(f"[V3] ì´ë¯¸ì§€ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
            self.blog_images = []

        # ì˜ìƒ ìˆ˜ì§‘ (Pexels Portrait + Pixabay + YouTube CC)
        video_kw = self.smart_keywords.get("video_keywords_en", [])
        search_en = video_kw[0] if video_kw else gen.translate_for_search(self.product_info["title"])
        try:
            self.video_sources = omni.collect_video_sources(
                product_title=self.product_info["title"],
                search_keyword_en=search_en,
                count=6,
            )
        except Exception:
            self.video_sources = []

        # ì†Œì…œ URL ì§ì ‘ ì¶”ì¶œ (TikTok/Instagram/YouTube)
        social_count = 0
        if self.social_urls:
            mc = MediaCollector()
            for url in self.social_urls:
                url = url.strip()
                if not url:
                    continue
                try:
                    path = mc.download_from_social(url, auto_wash=False)
                    if path:
                        self.video_sources.append({
                            "path": path, "source": "social_direct",
                            "platform": mc.detect_platform(url),
                            "license": "extracted",
                        })
                        social_count += 1
                except Exception as e:
                    print(f"[V3] ì†Œì…œ URL ì¶”ì¶œ ì‹¤íŒ¨: {url[:40]}... {e}")

        detail = f"ì´ë¯¸ì§€ {len(self.blog_images)}ì¥ + ì˜ìƒ {len(self.video_sources)}ê°œ"
        if social_count:
            detail += f" (ì†Œì…œ {social_count}ê°œ í¬í•¨)"
        self._emit(3, "collect", "complete", detail)

    # â”€â”€ Step 4: AI ë¯¸ë””ì–´ ìƒì„± (NanoBanana + Imagen + VEO) â”€â”€
    def _step_4_ai_generate(self):
        self._emit(4, "ai_media", "running", "AI ì´ë¯¸ì§€/ì˜ìƒ ìƒì„± ì¤‘ (ë§ˆì´í¬ë¡œ í”„ë¡¬í”„íŠ¸)...")
        from affiliate_system.ai_generator import AIGenerator
        from affiliate_system.config import V2_BLOG_DIR
        gen = AIGenerator()
        ai_output_dir = str(V2_BLOG_DIR / "v3_ai_generated")
        os.makedirs(ai_output_dir, exist_ok=True)

        # í”„ë¡¬í”„íŠ¸ ê°•í™”
        base_prompts = self.smart_keywords.get("ai_image_prompts", [])
        if not base_prompts:
            # í´ë°± í”„ë¡¬í”„íŠ¸
            en_name = gen.translate_to_english(self.product_info["title"])
            base_prompts = [
                f"Photorealistic {en_name} lifestyle product shot",
                f"Korean model using {en_name}, candid lifestyle moment",
                f"{en_name} product detail close-up, studio setting",
            ]
        enhanced = self._enhance_ai_prompts(base_prompts[:3])

        # 1) Gemini Imagen 4.0 â€” ì„¸ë¡œ ì´ë¯¸ì§€ 3ì¥
        imagen_count = 0
        try:
            imagen_images = gen.generate_ai_images(
                prompts=enhanced[:3], output_dir=ai_output_dir,
                count_per_prompt=1, aspect_ratio="9:16",
            )
            self.ai_images.extend(imagen_images)
            imagen_count = len(imagen_images)
        except Exception as e:
            print(f"[V3] Imagen 4.0 ì—ëŸ¬: {e}")

        # 2) NanoBanana Pro â€” 4K ì´ë¯¸ì§€ 2ì¥
        nano_count = 0
        for i in range(2):
            prompt_idx = i % len(enhanced)
            out_path = os.path.join(ai_output_dir, f"nano_{self.job_id}_{i}.png")
            result = self._call_nano_banana(enhanced[prompt_idx], out_path)
            if result:
                self.ai_images.append(result)
                nano_count += 1

        # 3) VEO 3.1 â€” B-roll ì˜ìƒ 1ê°œ
        veo_count = 0
        en_name = gen.translate_to_english(self.product_info["title"])
        veo_prompt = (
            f"Cinematic B-roll of {en_name} product being used in daily life, "
            f"9:16 vertical, smooth camera movement, warm lighting, "
            f"Korean model, lifestyle setting, 8 seconds"
        )
        veo_path = os.path.join(ai_output_dir, f"veo_{self.job_id}.mp4")
        veo_result = self._call_veo(veo_prompt, veo_path)
        if veo_result:
            self.ai_videos.append(veo_result)
            self.video_sources.append({
                "path": veo_result, "source": "veo_3.1",
                "license": "ai_generated",
            })
            veo_count = 1

        # AI ì´ë¯¸ì§€ë¥¼ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ í’€ì— ì¶”ê°€
        self.blog_images.extend(self.ai_images)

        detail = f"Imagen {imagen_count}ì¥ + NanoBanana {nano_count}ì¥ + VEO {veo_count}ê°œ"
        self._emit(4, "ai_media", "complete", detail)

    # â”€â”€ Step 5: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìµœì í™” â”€â”€
    def _step_5_naver(self):
        self._emit(5, "naver", "running", "ë„¤ì´ë²„ ë¸”ë¡œê·¸ HTML ìµœì í™” ì¤‘ (ì´ë¯¸ì§€ 5-7ì¥ 860px)...")
        try:
            from affiliate_system.blog_html_generator import NaverBlogHTMLGenerator
            html_gen = NaverBlogHTMLGenerator()
            # ì´ë¯¸ì§€ 5-7ì¥ìœ¼ë¡œ ì œí•œ (860px ë¦¬ì‚¬ì´ì§•ì€ html_gen ë‚´ë¶€ ì²˜ë¦¬)
            valid_images = [p for p in self.blog_images if p and os.path.exists(str(p))][:7]
            self.blog_html = html_gen.generate_blog_html(
                title=self.blog_content.get("title", self.product_info.get("title", "")),
                intro=self.blog_content.get("intro", ""),
                body_sections=self.blog_content.get("body_sections", []),
                image_paths=valid_images,
                coupang_link=self.affiliate_link,
                hashtags=self.blog_content.get("hashtags", []),
                banner_tag=self.banner_tag,
            )
            self._emit(5, "naver", "complete",
                       f"HTML {len(self.blog_html)}ì + ì´ë¯¸ì§€ {len(valid_images)}ì¥ êµì°¨ë°°ì¹˜ + CTA 2íšŒ")
        except Exception as e:
            self._emit(5, "naver", "error", str(e))

    # â”€â”€ Step 6: ìœ íŠœë¸Œ ì‡¼ì¸  ìµœì í™” (60fps, 12Mbps, 45ì´ˆ) â”€â”€
    def _step_6_youtube(self):
        self._emit(6, "youtube", "running", "ìœ íŠœë¸Œ ì‡¼ì¸  ìµœì í™” ì¤‘ (1080x1920 60fps 12Mbps)...")
        try:
            self.yt_shorts_path = self._render_shorts_for_platform("youtube")
            if self.yt_shorts_path:
                size_mb = os.path.getsize(self.yt_shorts_path) / (1024*1024)
                self._emit(6, "youtube", "complete", f"ì‡¼ì¸  ì™„ë£Œ: {size_mb:.1f}MB (60fps/12Mbps)")
            else:
                self._emit(6, "youtube", "complete", "ì˜ìƒ ì†ŒìŠ¤ ë¶€ì¡± â€” ì‡¼ì¸  ìƒëµ")
        except Exception as e:
            self._emit(6, "youtube", "error", str(e))

    # â”€â”€ Step 7: ì¸ìŠ¤íƒ€ ë¦´ìŠ¤ ìµœì í™” (30fps, 10Mbps, 30ì´ˆ) â”€â”€
    def _step_7_instagram(self):
        self._emit(7, "instagram", "running", "ì¸ìŠ¤íƒ€ ë¦´ìŠ¤ ìµœì í™” ì¤‘ (1080x1920 30fps 10Mbps)...")
        try:
            self.ig_reels_path = self._render_shorts_for_platform("instagram")
            if self.ig_reels_path:
                size_mb = os.path.getsize(self.ig_reels_path) / (1024*1024)
                self._emit(7, "instagram", "complete", f"ë¦´ìŠ¤ ì™„ë£Œ: {size_mb:.1f}MB (30fps/10Mbps)")
            else:
                self._emit(7, "instagram", "complete", "ì˜ìƒ ì†ŒìŠ¤ ë¶€ì¡± â€” ë¦´ìŠ¤ ìƒëµ")
        except Exception as e:
            self._emit(7, "instagram", "error", str(e))

    def _render_shorts_for_platform(self, platform):
        """í”Œë«í¼ë³„ ìˆí¼ ë Œë”ë§ (YouTube 60fps vs Instagram 30fps)."""
        scenes_data = self.shorts_script.get("scenes", [])
        video_paths = [v["path"] for v in self.video_sources if v.get("path")]

        if not video_paths or not scenes_data:
            # ë¹„ë””ì˜¤ ì—†ìœ¼ë©´ ì´ë¯¸ì§€â†’ì˜ìƒ í´ë°±
            if self.blog_images:
                video_paths = self._images_to_clips(platform)
            if not video_paths:
                return None

        # ì˜ìƒ ì„¸íƒ
        from affiliate_system.video_launderer import VideoLaunderer
        launderer = VideoLaunderer()
        laundered = launderer.batch_launder(video_paths)
        if not laundered:
            laundered = video_paths

        # emotion ìœ íš¨ì„± ê²€ì¦
        valid_emotions = {"excited", "friendly", "urgent", "dramatic", "calm", "hyped"}
        for sd in scenes_data:
            if sd.get("emotion", "friendly") not in valid_emotions:
                sd["emotion"] = "friendly"

        # TTS + ìë§‰ ìƒì„±
        from affiliate_system.video_launderer import EmotionTTSEngine, SubtitleGenerator, ProShortsRenderer, ShortsRenderer
        from affiliate_system.config import V2_SHORTS_DIR
        tts_engine = EmotionTTSEngine()
        sub_id = f"{self.job_id}_{platform}"
        scenes = tts_engine.generate_scenes_tts(scenes_data, sub_id)
        sub_gen = SubtitleGenerator()
        subtitle_path = sub_gen.generate_ass_from_scenes(scenes, sub_id)

        # ì˜ìƒ-ì¥ë©´ ë§¤í•‘
        render_scenes = []
        for i, sc in enumerate(scenes):
            vid_idx = i % len(laundered)
            render_scenes.append({
                "video_clip_path": laundered[vid_idx],
                "tts_path": sc.get("tts_path", "") or "",
                "tts_duration": sc.get("tts_duration", sc.get("duration", 3.0)),
                "text": sc.get("text", ""),
                "emotion": sc.get("emotion", "friendly"),
            })

        # ë Œë”ë§ (ProShortsRenderer â†’ ShortsRenderer í´ë°±)
        product_name = self.product_info.get("title", "ìƒí’ˆ")
        category = self.smart_keywords.get("category_detected", "")
        try:
            renderer = ProShortsRenderer()
            result = renderer.render_pro_shorts(
                scenes=render_scenes, campaign_id=sub_id,
                subtitle_path=subtitle_path,
                product_name=product_name, category=category,
            )
        except Exception:
            renderer = ShortsRenderer()
            result = renderer.render_final_shorts(
                scenes=render_scenes, campaign_id=sub_id,
                subtitle_path=subtitle_path,
                coupang_link=self.affiliate_link,
            )

        if not result or not os.path.exists(str(result)):
            return None

        # í”Œë«í¼ë³„ FFmpeg í›„ì²˜ë¦¬ (fps + bitrate ì¡°ì •)
        import subprocess as sp
        from affiliate_system.config import FFMPEG_CRF
        output_dir = str(Path(V2_SHORTS_DIR) / f"v3_{platform}")
        os.makedirs(output_dir, exist_ok=True)
        final_path = os.path.join(output_dir, f"v3_{platform}_{self.job_id}.mp4")

        if platform == "youtube":
            # 60fps, 12Mbps
            cmd = [
                "ffmpeg", "-y", "-i", str(result),
                "-r", "60", "-b:v", "12M", "-maxrate", "14M", "-bufsize", "24M",
                "-c:v", "h264_nvenc", "-preset", "p4", "-pix_fmt", "yuv420p",
                "-c:a", "aac", "-b:a", "192k",
                final_path,
            ]
        else:
            # Instagram: 30fps, 10Mbps
            cmd = [
                "ffmpeg", "-y", "-i", str(result),
                "-r", "30", "-b:v", "10M", "-maxrate", "12M", "-bufsize", "20M",
                "-c:v", "h264_nvenc", "-preset", "p4", "-pix_fmt", "yuv420p",
                "-c:a", "aac", "-b:a", "192k",
                final_path,
            ]

        try:
            sp.run(cmd, capture_output=True, timeout=300)
            if os.path.exists(final_path) and os.path.getsize(final_path) > 10000:
                return final_path
        except Exception as e:
            print(f"[V3] FFmpeg í›„ì²˜ë¦¬ ì‹¤íŒ¨ ({platform}), ì›ë³¸ ì‚¬ìš©: {e}")

        # í›„ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
        return str(result)

    def _images_to_clips(self, platform):
        """ì´ë¯¸ì§€ â†’ ì˜ìƒ í´ë¦½ í´ë°± (Ken Burns íš¨ê³¼)."""
        import subprocess as sp
        from affiliate_system.config import V2_SHORTS_DIR, FFMPEG_CRF
        clip_dir = Path(V2_SHORTS_DIR) / f"v3_img_clips_{platform}"
        clip_dir.mkdir(parents=True, exist_ok=True)
        clips = []
        for i, img in enumerate(self.blog_images[:6]):
            if not img or not os.path.exists(str(img)):
                continue
            out = str(clip_dir / f"clip_{i}_{self.job_id}.mp4")
            try:
                sp.run([
                    "ffmpeg", "-y", "-loop", "1", "-i", str(img),
                    "-vf", (
                        "scale=1080:1920:force_original_aspect_ratio=increase,"
                        "crop=1080:1920,"
                        "zoompan=z='min(zoom+0.0015,1.3)':d=240:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':s=1080x1920:fps=30"
                    ),
                    "-t", "8", "-c:v", "libx264", "-crf", FFMPEG_CRF,
                    "-preset", "medium", "-pix_fmt", "yuv420p", "-an", out,
                ], capture_output=True, timeout=60)
                if os.path.exists(out) and os.path.getsize(out) > 10000:
                    clips.append(out)
            except Exception:
                pass
        return clips

    # â”€â”€ Step 8: ì—…ë¡œë“œ & ì•„ì¹´ì´ë¹™ â”€â”€
    def _step_8_deploy(self):
        self._emit(8, "deploy", "running", "í”Œë«í¼ë³„ ì—…ë¡œë“œ + Drive ì•„ì¹´ì´ë¹™ ì¤‘...")
        upload_results = {}

        # ê²°ê³¼ ì €ì¥
        self.results["blog_html"] = self.blog_html
        self.results["blog_images"] = self.blog_images
        self.results["ai_images"] = self.ai_images
        self.results["ai_videos"] = self.ai_videos
        self.results["yt_shorts"] = self.yt_shorts_path
        self.results["ig_reels"] = self.ig_reels_path
        self.results["video_sources_count"] = len(self.video_sources)

        # í”Œë«í¼ë³„ ì—…ë¡œë“œ
        any_upload = any([
            self.upload_flags.get("youtube"), self.upload_flags.get("instagram"),
            self.upload_flags.get("naver"),
        ])
        if any_upload:
            try:
                from affiliate_system.auto_uploader import StealthUploader
                uploader = StealthUploader()
                uploaded = []

                if self.upload_flags.get("youtube") and self.yt_shorts_path:
                    try:
                        if uploader.youtube_auth():
                            title = self.product_info["title"]
                            yt = uploader.youtube_upload_v2(
                                video_path=self.yt_shorts_path,
                                title=f"{title} ì¶”ì²œ #Shorts",
                                description=f"#{title} #ì¿ íŒ¡ #ì¶”ì²œ #ì‡¼ì¸ ",
                            )
                            if yt:
                                uploaded.append("YouTube")
                                upload_results["youtube"] = yt
                    except Exception as e:
                        upload_results["youtube_error"] = str(e)

                if self.upload_flags.get("instagram") and self.ig_reels_path:
                    try:
                        if uploader.instagram_auth():
                            title = self.product_info["title"]
                            ig = uploader.instagram_upload_reel_v2(
                                video_path=self.ig_reels_path,
                                caption=f"{title} ì†”ì§ ì¶”ì²œ! ğŸ’¯\n#ì¿ íŒ¡ #{title.replace(' ', '')} #ì¶”ì²œ",
                            )
                            if ig:
                                uploaded.append("Instagram")
                                upload_results["instagram"] = ig
                    except Exception as e:
                        upload_results["instagram_error"] = str(e)

                if self.upload_flags.get("naver") and self.blog_html:
                    try:
                        nv = uploader.naver_blog_post_v2(
                            html_content=self.blog_html,
                            title=self.product_info["title"],
                        )
                        if nv:
                            uploaded.append("Naver")
                            upload_results["naver"] = nv
                    except Exception as e:
                        upload_results["naver_error"] = str(e)

                self.results["upload_results"] = upload_results
            except Exception as e:
                print(f"[V3] ì—…ë¡œë” ë¡œë“œ ì‹¤íŒ¨: {e}")

        # Drive ì•„ì¹´ì´ë¹™
        if self.upload_flags.get("drive", True):
            try:
                from affiliate_system.drive_manager import DriveArchiver
                from affiliate_system.models import Campaign, AIContent, CampaignStatus, Product
                archiver = DriveArchiver()
                if archiver.authenticate():
                    valid_images = [p for p in self.blog_images if p and Path(p).exists()]
                    drive_files = {
                        "naver_blog": [],
                        "instagram_shorts": [],
                        "youtube_shorts": [],
                    }
                    # ë¸”ë¡œê·¸ HTML + ì´ë¯¸ì§€
                    if self.blog_html:
                        html_path = Path(WORK_DIR) / f"v3_blog_{self.job_id}.html"
                        html_path.write_text(self.blog_html, encoding="utf-8")
                        drive_files["naver_blog"].append(str(html_path))
                    drive_files["naver_blog"].extend([str(p) for p in valid_images])
                    # ìˆí¼ ì˜ìƒ
                    if self.ig_reels_path and Path(self.ig_reels_path).exists():
                        drive_files["instagram_shorts"].append(self.ig_reels_path)
                    if self.yt_shorts_path and Path(self.yt_shorts_path).exists():
                        drive_files["youtube_shorts"].append(self.yt_shorts_path)

                    temp_product = Product(
                        title=self.product_info["title"],
                        description=self.product_info.get("description", ""),
                        url=self.coupang_url, affiliate_link=self.affiliate_link,
                    )
                    temp_campaign = Campaign(
                        id=self.job_id, product=temp_product,
                        ai_content=AIContent(platform_contents={}),
                        status=CampaignStatus.COMPLETE,
                        target_platforms=[], platform_videos={},
                        platform_thumbnails={}, created_at=datetime.now(),
                    )
                    archive_result = archiver.archive_campaign(temp_campaign, drive_files, v2=True)
                    if archive_result["ok"]:
                        self.results["drive_url"] = archive_result.get("folder_url", "")
                        self.results["drive_platforms"] = archive_result.get("platform_urls", {})
            except Exception as e:
                print(f"[V3] Drive ì•„ì¹´ì´ë¹™ ì—ëŸ¬: {e}")

        detail_parts = []
        if upload_results:
            uploaded = [k for k, v in upload_results.items() if not k.endswith("_error") and v]
            detail_parts.append(f"ì—…ë¡œë“œ: {', '.join(uploaded) if uploaded else 'ì—†ìŒ'}")
        if self.results.get("drive_url"):
            detail_parts.append("Drive ì•„ì¹´ì´ë¹™ ì™„ë£Œ")
        self._emit(8, "deploy", "complete", " | ".join(detail_parts) or "ì™„ë£Œ")


# â”€â”€ V3 API ì—”ë“œí¬ì¸íŠ¸ â”€â”€

@app.route('/api/v3/steps')
def v3_steps():
    return jsonify(V3_STEPS)


@app.route('/api/v3/campaign/start', methods=['POST'])
def v3_start_campaign():
    _cleanup_old_jobs(v3_jobs)  # ì˜¤ë˜ëœ ì¡ ì •ë¦¬

    data = request.json or {}
    coupang_url = data.get("coupang_url", "").strip()
    affiliate_link = data.get("affiliate_link", "").strip()
    if not coupang_url or not affiliate_link:
        return jsonify({"error": "ì¿ íŒ¡ ìƒí’ˆ URLê³¼ ì œíœ´ ë§í¬ í•„ìˆ˜"}), 400

    job_id = uuid.uuid4().hex[:12]
    events_queue = Queue()

    pipeline = V3WebPipeline(
        job_id=job_id,
        topic=data.get("topic", ""),
        coupang_url=coupang_url,
        affiliate_link=affiliate_link,
        banner_tag=data.get("banner_tag", ""),
        product_name=data.get("product_name", ""),
        ai_provider=data.get("ai_provider", ""),
        review_mode=data.get("review_mode", False),
        upload_flags=data.get("upload_flags", {"youtube": False, "instagram": False, "naver": False, "drive": True}),
        social_urls=data.get("social_urls", []),
        events_queue=events_queue,
    )

    v3_jobs[job_id] = {
        "state": V3PipelineState.ANALYZING,
        "pipeline": pipeline,
        "events": events_queue,
        "created_at": datetime.now().isoformat(),
        "results": {},
        "error": None,
    }

    def worker():
        job = v3_jobs[job_id]
        try:
            result_state = pipeline.run()
            if result_state == "awaiting_confirm":
                job["state"] = V3PipelineState.AWAITING_CONFIRM
                job["events"].put({
                    "type": "state_change",
                    "state": V3PipelineState.AWAITING_CONFIRM,
                    "message": "âœ… ì´ˆì•ˆ ìƒì„± ì™„ë£Œ! í™•ì¸ í›„ ê³„ì† ì§„í–‰í•˜ì„¸ìš”.",
                    "draft": {
                        "blog": _safe_serialize(pipeline.blog_content) if hasattr(pipeline, 'blog_content') else {},
                        "shorts": _safe_serialize(pipeline.shorts_script) if hasattr(pipeline, 'shorts_script') else {},
                        "product": _safe_serialize(pipeline.product_info) if hasattr(pipeline, 'product_info') else {},
                    },
                    "timestamp": datetime.now().isoformat(),
                })
            else:
                job["state"] = V3PipelineState.COMPLETE
                job["results"] = pipeline.results
                job["events"].put({
                    "type": "v3_complete",
                    "results": _safe_serialize(pipeline.results),
                    "timestamp": datetime.now().isoformat(),
                })
                _save_campaign(job_id, pipeline.product_info.get("title", "V3"),
                               "V3", ["naver_blog", "youtube", "instagram"], "complete",
                               results=_safe_serialize(pipeline.results))
        except Exception as e:
            job["state"] = V3PipelineState.ERROR
            job["error"] = str(e)
            job["events"].put({"type": "error", "error": str(e), "timestamp": datetime.now().isoformat()})

    threading.Thread(target=worker, daemon=True).start()
    return jsonify({"job_id": job_id, "status": "started"})


@app.route('/api/v3/campaign/<job_id>/confirm', methods=['POST'])
def v3_confirm(job_id):
    job = v3_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    if job["state"] != V3PipelineState.AWAITING_CONFIRM:
        return jsonify({"error": f"í˜„ì¬ ìƒíƒœ: {job['state']}"}), 400

    # ì—…ë¡œë“œ í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
    confirm_data = request.json or {}
    if "upload_flags" in confirm_data:
        job["pipeline"].upload_flags = confirm_data["upload_flags"]

    job["state"] = V3PipelineState.EXECUTING
    job["events"].put({
        "type": "state_change", "state": V3PipelineState.EXECUTING,
        "message": "ğŸš€ 3~8ë‹¨ê³„ ì‹¤í–‰ ì‹œì‘!",
        "timestamp": datetime.now().isoformat(),
    })

    def resume():
        try:
            job["pipeline"].resume_after_confirm()
            job["state"] = V3PipelineState.COMPLETE
            job["results"] = job["pipeline"].results
            job["events"].put({
                "type": "v3_complete",
                "results": _safe_serialize(job["pipeline"].results),
                "timestamp": datetime.now().isoformat(),
            })
            _save_campaign(job_id, job["pipeline"].product_info.get("title", "V3"),
                           "V3", ["naver_blog", "youtube", "instagram"], "complete",
                           results=_safe_serialize(job["pipeline"].results))
        except Exception as e:
            job["state"] = V3PipelineState.ERROR
            job["error"] = str(e)
            job["events"].put({"type": "error", "error": str(e), "timestamp": datetime.now().isoformat()})

    threading.Thread(target=resume, daemon=True).start()
    return jsonify({"job_id": job_id, "state": V3PipelineState.EXECUTING})


@app.route('/api/v3/campaign/stream/<job_id>')
@app.route('/api/v3/campaign/<job_id>/stream')
def v3_stream(job_id):
    def generate():
        job = v3_jobs.get(job_id)
        if not job:
            yield f"data: {json.dumps({'type': 'error', 'error': 'Job not found'})}\n\n"
            return
        q = job["events"]
        # ì¢…ë£Œ ì¡°ê±´: COMPLETE ë˜ëŠ” ERROR (AWAITING_CONFIRMì—ì„œëŠ” ëŠê³ , ì¬ì—°ê²° ëŒ€ê¸°)
        stop_states = (V3PipelineState.COMPLETE, V3PipelineState.ERROR)
        pause_states = (V3PipelineState.AWAITING_CONFIRM,)
        timeout_count = 0
        max_idle = 300  # 90ì´ˆ ëŒ€ê¸° í›„ ì¢…ë£Œ (0.3s * 300)
        while True:
            state = job["state"]
            if state in stop_states:
                break
            if state in pause_states:
                # AWAITING_CONFIRM: ë‚¨ì€ ì´ë²¤íŠ¸ í”ŒëŸ¬ì‹œ í›„ ì¢…ë£Œ (í”„ë¡ íŠ¸ì—ì„œ confirm í›„ ì¬ì—°ê²°)
                while True:
                    try:
                        event = q.get_nowait()
                    except Exception:
                        break
                    yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"
                break
            # ì´ë²¤íŠ¸ ì½ê¸°
            had_event = False
            while True:
                try:
                    event = q.get_nowait()
                except Exception:
                    break
                yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"
                had_event = True
                timeout_count = 0
            if not had_event:
                timeout_count += 1
                if timeout_count >= max_idle:
                    yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                    timeout_count = 0
            time.sleep(0.3)
        # ìµœì¢… í”ŒëŸ¬ì‹œ
        while True:
            try:
                event = q.get_nowait()
            except Exception:
                break
            yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"
        if job["state"] == V3PipelineState.COMPLETE:
            yield f"data: {json.dumps({'type': 'v3_done', 'results': _safe_serialize(job.get('results', {}))}, ensure_ascii=False, default=str)}\n\n"
        elif job["state"] == V3PipelineState.ERROR:
            yield f"data: {json.dumps({'type': 'error', 'error': job.get('error', '')})}\n\n"
    return Response(generate(), mimetype='text/event-stream',
                    headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})


@app.route('/api/v3/campaign/<job_id>/status')
def v3_status(job_id):
    job = v3_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    pipeline = job.get("pipeline")
    return jsonify({
        "job_id": job_id,
        "state": job["state"],
        "product_info": pipeline.product_info if pipeline else {},
        "draft": {"blog": pipeline.blog_content, "shorts": pipeline.shorts_script} if pipeline else {},
        "results": _safe_serialize(job.get("results", {})),
        "error": job.get("error"),
    })


@app.route('/api/v3/campaign/<job_id>/blog-preview')
def v3_blog_preview(job_id):
    job = v3_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    pipeline = job.get("pipeline")
    if pipeline and pipeline.blog_html:
        return Response(pipeline.blog_html, mimetype='text/html; charset=utf-8')
    return jsonify({"error": "ë¸”ë¡œê·¸ HTML ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ"}), 404


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == '__main__':
    import io, sys
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    _start_periodic_cleanup()  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‹œì‘
    print("=" * 50)
    print("YJ MCN Automation Dashboard Server V3.1")
    print(f"  URL: http://localhost:5001")
    print(f"  Gemini: {'OK' if GEMINI_API_KEY else 'NO'}")
    print(f"  Claude: {'OK' if ANTHROPIC_API_KEY else 'NO'}")
    print(f"  Pexels: {'OK' if PEXELS_API_KEY else 'NO'}")
    print("=" * 50)
    app.run(host='127.0.0.1', port=5001, debug=False, threaded=True)
