# -*- coding: utf-8 -*-
"""
YJ MCN ìë™í™” ëŒ€ì‹œë³´ë“œ â€” Flask ë°±ì—”ë“œ
=====================================
ê¸°ì¡´ affiliate_system ëª¨ë“ˆì„ REST APIë¡œ ë˜í•‘.
SSEë¡œ íŒŒì´í”„ë¼ì¸ ì§„í–‰ìƒí™© ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°.

ì‹¤í–‰: python yj-partners-mcn/mcn_server.py
"""
import json
import os
import sys
import time
import uuid
import threading
from pathlib import Path
from queue import Queue
from datetime import datetime

from flask import Flask, request, jsonify, Response, send_file, send_from_directory
from flask_cors import CORS
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
PROJECT_DIR = Path(__file__).parent.parent  # franchise-db/
sys.path.insert(0, str(PROJECT_DIR))
load_dotenv(PROJECT_DIR / '.env', override=True)

from affiliate_system.config import (
    GEMINI_API_KEY, ANTHROPIC_API_KEY, PEXELS_API_KEY,
    PIXABAY_API_KEY, UNSPLASH_ACCESS_KEY, RENDER_OUTPUT_DIR, WORK_DIR,
)
from affiliate_system.models import Platform, PLATFORM_PRESETS

# â”€â”€ ì»¤ë§¨ë“œì„¼í„° AI ì„œë¹„ìŠ¤ ì—°ë™ â”€â”€
sys.path.insert(0, str(PROJECT_DIR))
from command_center.config import OPENAI_API_KEY, OLLAMA_BASE_URL, OLLAMA_MODEL, AI_PROVIDERS
from command_center.services.ai_service import AIService

ai_service = AIService()

# â”€â”€ Flask ì•± ì„¤ì • â”€â”€
app = Flask(__name__, static_folder=str(Path(__file__).parent))
CORS(app)

# â”€â”€ Job ì €ì¥ì†Œ & ìº í˜ì¸ íˆìŠ¤í† ë¦¬ â”€â”€
jobs = {}  # job_id -> {status, step, progress, results, events, error}
campaign_history = []  # ìº í˜ì¸ ì´ë ¥ (ìµœê·¼ 50ê°œ)

# â”€â”€ ìº í˜ì¸ DB (SQLite) â”€â”€
import sqlite3
CAMPAIGN_DB = str(PROJECT_DIR / "mcn_campaigns.db")

def _init_campaign_db():
    """ìº í˜ì¸ íˆìŠ¤í† ë¦¬ DB ì´ˆê¸°í™”"""
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.execute("""CREATE TABLE IF NOT EXISTS campaigns (
        id TEXT PRIMARY KEY,
        topic TEXT, brand TEXT, platforms TEXT,
        ai_provider TEXT, cost_usd REAL DEFAULT 0,
        status TEXT DEFAULT 'pending',
        results TEXT,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP
    )""")
    conn.commit()
    conn.close()

_init_campaign_db()

# â”€â”€ ë¸Œëœë“œ ì„¤ì • â”€â”€
BRANDS = {
    "ì˜¤ë ˆë…¸ì¹´ì¸ ": {
        "name": "ì˜¤ë ˆë…¸ì¹´ì¸ ",
        "type": "Japanese Tonkatsu Franchise",
        "emoji": "ğŸ±",
        "platforms": ["youtube", "naver_blog", "instagram"],
        "campaigns": 12,
    },
    "ë¬´ì‚¬ì§¬ë½•": {
        "name": "ë¬´ì‚¬ì§¬ë½•",
        "type": "Chinese Jjamppong Franchise",
        "emoji": "ğŸœ",
        "platforms": ["youtube", "naver_blog", "instagram"],
        "campaigns": 8,
    },
    "ë¸Œë¦¿ì§€ì›": {
        "name": "ë¸Œë¦¿ì§€ì›",
        "type": "Franchise Consulting (BRIDGE ONE)",
        "emoji": "ğŸ’¼",
        "platforms": ["youtube", "naver_blog", "instagram", "coupang"],
        "campaigns": 15,
    },
}

PLATFORM_MAP = {
    "youtube": Platform.YOUTUBE,
    "instagram": Platform.INSTAGRAM,
    "naver_blog": Platform.NAVER_BLOG,
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _to_relative_path(abs_path) -> str:
    """ì ˆëŒ€ ê²½ë¡œë¥¼ PROJECT_DIR ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜ (í”„ë¡ íŠ¸ì—”ë“œ íŒŒì¼ ì„œë¹™ìš©)."""
    if not abs_path:
        return ""
    p = Path(str(abs_path))
    try:
        return str(p.relative_to(PROJECT_DIR)).replace("\\", "/")
    except ValueError:
        # PROJECT_DIR ë°–ì˜ ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return str(p).replace("\\", "/")


def serialize_results(results: dict) -> dict:
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜."""
    out = {"platforms": {}, "upload_results": {}}
    for p_name, p_data in results.get("platforms", {}).items():
        out["platforms"][p_name] = {
            "video": _to_relative_path(p_data.get("video")) or None,
            "thumbnail": _to_relative_path(p_data.get("thumbnail")) or None,
            "content": p_data.get("content", {}),
        }
    out["upload_results"] = results.get("upload_results", {})
    campaign = results.get("campaign")
    if campaign:
        out["campaign_id"] = campaign.id
        out["created_at"] = campaign.created_at.isoformat() if campaign.created_at else None
    return out


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# WebPipeline â€” ë‹¨ê³„ë³„ SSE ì´ë²¤íŠ¸ ë°œìƒ ë˜í¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class WebPipeline:
    """ContentPipelineì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•˜ë©° SSE ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œí‚¨ë‹¤."""

    def __init__(self, events_queue: Queue):
        self._q = events_queue

    def _emit(self, step: int, name: str, status: str, detail: str = ""):
        self._q.put({
            "type": "step",
            "step": step,
            "name": name,
            "status": status,
            "detail": detail,
            "timestamp": datetime.now().isoformat(),
        })

    def run(self, topic: str, platforms: list, brand: str, persona: str,
            auto_upload: bool, drive_archive: bool = True) -> dict:
        from affiliate_system.pipeline import ContentPipeline

        pipeline = ContentPipeline()
        platform_enums = [PLATFORM_MAP[p] for p in platforms if p in PLATFORM_MAP]
        if not platform_enums:
            platform_enums = list(PLATFORM_MAP.values())

        results = {"platforms": {}, "upload_results": {}}

        # Step 1: ìƒí’ˆ ì •ë³´
        self._emit(1, "product_info", "running", "ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        try:
            product = pipeline._prepare_product(topic)
            self._emit(1, "product_info", "complete", f"ìƒí’ˆ: {product.title}")
        except Exception as e:
            self._emit(1, "product_info", "error", str(e))
            raise

        # Step 2: AI ì½˜í…ì¸  ìƒì„±
        self._emit(2, "ai_content", "running", f"{len(platform_enums)}ê°œ í”Œë«í¼ AI ì½˜í…ì¸  ìƒì„± ì¤‘...")
        try:
            platform_contents = pipeline._generate_contents(product, platform_enums, persona, brand)
            detail_parts = []
            for p_name, content in platform_contents.items():
                narr = len(content.get("narration", []))
                tags = len(content.get("hashtags", []))
                detail_parts.append(f"{p_name}: ë‚˜ë ˆì´ì…˜ {narr}ì¥ë©´, íƒœê·¸ {tags}ê°œ")
            self._emit(2, "ai_content", "complete", " | ".join(detail_parts))
        except Exception as e:
            self._emit(2, "ai_content", "error", str(e))
            raise

        # Step 3: ë¯¸ë””ì–´ ìˆ˜ì§‘
        self._emit(3, "media", "running", "ë¬´ë£Œ ìŠ¤í†¡ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì¤‘...")
        try:
            images = pipeline._collect_media(product)
            self._emit(3, "media", "complete", f"ì´ë¯¸ì§€ {len(images)}ê°œ ìˆ˜ì§‘")
        except Exception as e:
            self._emit(3, "media", "error", str(e))
            images = []

        # Step 4: ì¸ë„¤ì¼ ìƒì„±
        self._emit(4, "thumbnail", "running", "í”Œë«í¼ë³„ ì¸ë„¤ì¼ ìƒì„± ì¤‘...")
        try:
            campaign_id = uuid.uuid4().hex[:8]
            thumbnails = pipeline._generate_thumbnails(
                platform_enums, platform_contents, images, brand, campaign_id,
            )
            self._emit(4, "thumbnail", "complete", f"{len(thumbnails)}ê°œ ì¸ë„¤ì¼ ìƒì„±")
        except Exception as e:
            self._emit(4, "thumbnail", "error", str(e))
            thumbnails = {}

        # Step 5: ì˜ìƒ ë Œë”ë§
        self._emit(5, "video_render", "running", "ì˜ìƒ ë Œë”ë§ ì¤‘ (ì‹œê°„ ì†Œìš”)...")
        try:
            videos = pipeline._render_videos(
                platform_enums, platform_contents, images, brand, campaign_id,
            )
            rendered = [p for p, v in videos.items() if v]
            self._emit(5, "video_render", "complete", f"{len(rendered)}ê°œ ì˜ìƒ ë Œë”ë§ ì™„ë£Œ")
        except Exception as e:
            self._emit(5, "video_render", "error", str(e))
            videos = {}

        # ê²°ê³¼ ì¡°í•© (ì ˆëŒ€ ê²½ë¡œ â†’ ìƒëŒ€ ê²½ë¡œ ë³€í™˜)
        for p in platform_enums:
            p_name = p.value
            results["platforms"][p_name] = {
                "video": _to_relative_path(videos.get(p_name)) or None,
                "thumbnail": _to_relative_path(thumbnails.get(p_name)) or None,
                "content": platform_contents.get(p_name, {}),
            }

        # Step 6: ì†Œì…œ ë¯¸ë””ì–´ ì—…ë¡œë“œ
        if auto_upload:
            self._emit(6, "upload", "running", "3í”Œë«í¼ ì—…ë¡œë“œ ì¤‘...")
            try:
                from affiliate_system.models import Campaign, AIContent, CampaignStatus
                campaign_obj = Campaign(
                    id=campaign_id, product=product,
                    ai_content=AIContent(platform_contents=platform_contents),
                    status=CampaignStatus.UPLOADING,
                    target_platforms=platform_enums,
                    platform_videos=videos, platform_thumbnails=thumbnails,
                    created_at=datetime.now(),
                )
                upload_results = pipeline._upload_all(campaign_obj)
                results["upload_results"] = upload_results
                self._emit(6, "upload", "complete", "ì—…ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                self._emit(6, "upload", "error", str(e))
        else:
            self._emit(6, "upload", "skipped", "ì—…ë¡œë“œ ê±´ë„ˆëœ€ (ìˆ˜ë™ ëª¨ë“œ)")

        # Step 7: Google Drive ìë™ ì•„ì¹´ì´ë¹™
        if drive_archive:
            self._emit(7, "drive_archive", "running", "Google Drive í´ë” ìƒì„± ë° ì—…ë¡œë“œ ì¤‘...")
            try:
                from affiliate_system.drive_manager import DriveArchiver
                from affiliate_system.models import Campaign, AIContent, CampaignStatus

                # Campaign ê°ì²´ ìƒì„±
                campaign_obj = Campaign(
                    id=campaign_id, product=product,
                    ai_content=AIContent(platform_contents=platform_contents),
                    status=CampaignStatus.COMPLETE,
                    target_platforms=platform_enums,
                    platform_videos=videos, platform_thumbnails=thumbnails,
                    created_at=datetime.now(),
                )

                # ì—…ë¡œë“œí•  íŒŒì¼ ë¶„ë¥˜
                drive_files = {"images": [], "renders": [], "audio": [], "logs": []}

                # ë Œë”ë§ ê²°ê³¼ (ì˜ìƒ + ì¸ë„¤ì¼)
                for p_name, v_path in videos.items():
                    if v_path and Path(str(v_path)).exists():
                        drive_files["renders"].append(str(v_path))
                for p_name, t_path in thumbnails.items():
                    if t_path and Path(str(t_path)).exists():
                        drive_files["renders"].append(str(t_path))

                # ìˆ˜ì§‘ëœ ì´ë¯¸ì§€
                media_dir = WORK_DIR / "media_downloads"
                if media_dir.exists():
                    for img in sorted(media_dir.glob("*.*"))[-20:]:  # ìµœê·¼ 20ê°œ
                        if img.suffix.lower() in (".jpg", ".jpeg", ".png", ".webp"):
                            drive_files["images"].append(str(img))

                # TTS ì˜¤ë””ì˜¤ (ìµœê·¼ ìƒì„±ëœ tts_ í´ë”)
                for tts_dir in sorted(WORK_DIR.glob("tts_*"), reverse=True)[:1]:
                    if tts_dir.is_dir():
                        for audio_f in tts_dir.glob("*.mp3"):
                            drive_files["audio"].append(str(audio_f))

                total_files = sum(len(v) for v in drive_files.values())
                self._emit(7, "drive_archive", "running",
                           f"{total_files}ê°œ íŒŒì¼ Drive ì—…ë¡œë“œ ì¤‘...")

                # DriveArchiverë¡œ ì—…ë¡œë“œ
                archiver = DriveArchiver()
                if archiver.authenticate():
                    def _progress(cur, tot, fname):
                        self._emit(7, "drive_archive", "running",
                                   f"Drive ì—…ë¡œë“œ {cur}/{tot}: {fname}")

                    archive_result = archiver.archive_campaign(
                        campaign_obj, drive_files, progress_callback=_progress
                    )

                    if archive_result["ok"]:
                        folder_url = archive_result.get("folder_url", "")
                        uploaded = archive_result["files_uploaded"]
                        self._emit(7, "drive_archive", "complete",
                                   f"Drive ì•„ì¹´ì´ë¹™ ì™„ë£Œ: {uploaded}ê°œ íŒŒì¼ ì—…ë¡œë“œ")
                        results["drive_url"] = folder_url
                        results["drive_files_uploaded"] = uploaded
                    else:
                        errors = archive_result.get("errors", [])
                        self._emit(7, "drive_archive", "error",
                                   f"ì¼ë¶€ ì‹¤íŒ¨: {', '.join(errors[:3])}")
                else:
                    self._emit(7, "drive_archive", "error",
                               "Drive ì¸ì¦ ì‹¤íŒ¨ â€” OAuth í† í°ì„ í™•ì¸í•˜ì„¸ìš”")
            except Exception as e:
                self._emit(7, "drive_archive", "error", str(e))
        else:
            self._emit(7, "drive_archive", "skipped", "Drive ì•„ì¹´ì´ë¹™ ê±´ë„ˆëœ€")

        results["campaign_id"] = campaign_id
        return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ì—”ë“œí¬ì¸íŠ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ ë©”ì¸ í˜ì´ì§€ â”€â”€
@app.route('/')
def index():
    return send_file(str(Path(__file__).parent / 'index.html'))


# â”€â”€ ê±´ê°• ì²´í¬ (AI 8ê°œ + ë¯¸ë””ì–´ + OpenClaw) â”€â”€
@app.route('/api/health')
def health():
    # AI í”„ë¡œë°”ì´ë” ìƒíƒœ
    providers = ai_service.list_providers()
    services = {}
    for p in providers:
        services[p["name"]] = p["available"]
    # ë¯¸ë””ì–´ API
    services["pexels"] = bool(PEXELS_API_KEY)
    services["pixabay"] = bool(PIXABAY_API_KEY)
    services["unsplash"] = bool(UNSPLASH_ACCESS_KEY)
    # OpenClaw ê²Œì´íŠ¸ì›¨ì´
    try:
        import requests as req
        oc = req.get("http://127.0.0.1:18792/__openclaw__/health", timeout=2)
        services["openclaw"] = oc.status_code == 200
    except Exception:
        services["openclaw"] = False

    return jsonify({
        "status": "online",
        "services": services,
        "active_jobs": sum(1 for j in jobs.values() if j["status"] == "running"),
        "ai_providers": providers,
        "timestamp": datetime.now().isoformat(),
    })


# â”€â”€ ë¸Œëœë“œ ëª©ë¡ â”€â”€
@app.route('/api/brands')
def get_brands():
    return jsonify(BRANDS)


# â”€â”€ ìº í˜ì¸ ì‹œì‘ â”€â”€
@app.route('/api/campaign/start', methods=['POST'])
def start_campaign():
    data = request.json or {}
    topic = data.get("topic", "").strip()
    if not topic:
        return jsonify({"error": "topic í•„ìˆ˜"}), 400

    brand = data.get("brand", "")
    platforms = data.get("platforms", ["youtube", "instagram", "naver_blog"])
    persona = data.get("persona", "")
    auto_upload = data.get("auto_upload", False)
    drive_archive = data.get("drive_archive", True)  # ê¸°ë³¸ ON

    job_id = uuid.uuid4().hex[:12]
    events_queue = Queue()

    jobs[job_id] = {
        "status": "pending",
        "step": 0,
        "topic": topic,
        "brand": brand,
        "platforms": platforms,
        "results": None,
        "error": None,
        "events": events_queue,
        "created_at": datetime.now().isoformat(),
    }

    # ìº í˜ì¸ íˆìŠ¤í† ë¦¬ ì €ì¥ (ì‹œì‘)
    _save_campaign(job_id, topic, brand, platforms, "running")

    def worker():
        job = jobs[job_id]
        job["status"] = "running"
        try:
            wp = WebPipeline(events_queue)
            results = wp.run(topic, platforms, brand, persona, auto_upload, drive_archive)
            job["results"] = results
            job["status"] = "complete"
            events_queue.put({"type": "complete", "results": results})
            # ìº í˜ì¸ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ì™„ë£Œ)
            _save_campaign(job_id, topic, brand, platforms, "complete",
                           results=_safe_serialize(results))
        except Exception as e:
            job["error"] = str(e)
            job["status"] = "error"
            events_queue.put({"type": "error", "error": str(e)})
            _save_campaign(job_id, topic, brand, platforms, "error")

    thread = threading.Thread(target=worker, daemon=True)
    thread.start()

    return jsonify({"job_id": job_id, "status": "started"})


# â”€â”€ SSE ì§„í–‰ìƒí™© ìŠ¤íŠ¸ë¦¬ë° â”€â”€
@app.route('/api/campaign/stream/<job_id>')
def stream_campaign(job_id):
    def generate():
        job = jobs.get(job_id)
        if not job:
            yield f"data: {json.dumps({'type': 'error', 'error': 'Job not found'})}\n\n"
            return

        q = job["events"]
        while job["status"] in ("pending", "running"):
            while not q.empty():
                event = q.get_nowait()
                # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
                if event.get("type") == "complete" and event.get("results"):
                    event["results"] = _safe_serialize(event["results"])
                yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"
            time.sleep(0.3)

        # ì”ì—¬ ì´ë²¤íŠ¸ flush
        while not q.empty():
            event = q.get_nowait()
            if event.get("type") == "complete" and event.get("results"):
                event["results"] = _safe_serialize(event["results"])
            yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"

        # ìµœì¢… ìƒíƒœ
        if job["status"] == "complete" and job["results"]:
            yield f"data: {json.dumps({'type': 'done', 'results': _safe_serialize(job['results'])}, ensure_ascii=False, default=str)}\n\n"
        elif job["status"] == "error":
            yield f"data: {json.dumps({'type': 'error', 'error': job['error']})}\n\n"

    return Response(generate(), mimetype='text/event-stream',
                    headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})


def _safe_serialize(obj):
    """ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜."""
    if isinstance(obj, dict):
        return {k: _safe_serialize(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_safe_serialize(v) for v in obj]
    if isinstance(obj, Path):
        return str(obj)
    if hasattr(obj, '__dict__') and not isinstance(obj, type):
        return str(obj)
    return obj


# â”€â”€ AI ì½˜í…ì¸ ë§Œ ìƒì„± (ì˜ìƒ ì—†ì´) â”€â”€
@app.route('/api/content/generate', methods=['POST'])
def generate_content():
    data = request.json or {}
    topic = data.get("topic", "").strip()
    if not topic:
        return jsonify({"error": "topic í•„ìˆ˜"}), 400

    brand = data.get("brand", "")
    persona = data.get("persona", "")
    platforms = data.get("platforms", ["youtube", "instagram", "naver_blog"])

    try:
        from affiliate_system.pipeline import ContentPipeline
        pipeline = ContentPipeline()
        product = pipeline._prepare_product(topic)

        platform_enums = [PLATFORM_MAP[p] for p in platforms if p in PLATFORM_MAP]
        contents = pipeline._generate_contents(product, platform_enums, persona, brand)

        return jsonify({
            "product": {"title": product.title, "description": product.description},
            "contents": contents,
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ë¬´ë£Œ ë¯¸ë””ì–´ ê²€ìƒ‰ â”€â”€
@app.route('/api/media/search', methods=['POST'])
def search_media():
    data = request.json or {}
    query = data.get("query", "").strip()
    media_type = data.get("type", "image")  # image or video

    if not query:
        return jsonify({"error": "query í•„ìˆ˜"}), 400

    try:
        from affiliate_system.media_collector import MediaCollector
        mc = MediaCollector()

        if media_type == "video":
            results = mc.search_videos(query, count=6)
        else:
            results = mc.search_images(query, count=12)

        return jsonify({"results": results, "query": query, "type": media_type})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ì†Œì…œ ë¯¸ë””ì–´ ë‹¤ìš´ë¡œë“œ (TikTok, YouTube ë“±) â”€â”€
@app.route('/api/media/social', methods=['POST'])
def download_social():
    data = request.json or {}
    url = data.get("url", "").strip()

    if not url:
        return jsonify({"error": "url í•„ìˆ˜"}), 400

    try:
        from affiliate_system.media_collector import MediaCollector
        mc = MediaCollector()
        result = mc.download_from_social(url)
        return jsonify({"result": result})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ AI ì´ë¯¸ì§€ ìƒì„± (Gemini) â”€â”€
@app.route('/api/media/ai-generate', methods=['POST'])
def ai_generate():
    data = request.json or {}
    prompt = data.get("prompt", "").strip()
    media_type = data.get("type", "image")

    if not prompt:
        return jsonify({"error": "prompt í•„ìˆ˜"}), 400

    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)

        if media_type == "image":
            # Gemini ì´ë¯¸ì§€ ìƒì„±
            model = genai.GenerativeModel('gemini-2.0-flash-exp')
            response = model.generate_content(prompt)
            # í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
            return jsonify({
                "type": "image",
                "prompt_used": prompt,
                "response": response.text if response.text else "ìƒì„± ì™„ë£Œ",
            })
        else:
            return jsonify({"error": "video AI ìƒì„±ì€ ì¤€ë¹„ ì¤‘"}), 501
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ë¹„ìš© í˜„í™© (ì»¤ë§¨ë“œì„¼í„° ì—°ë™) â”€â”€
@app.route('/api/cost')
def get_cost():
    try:
        from api_cost_tracker import CostTracker
        tracker = CostTracker()
        summary = tracker.get_summary()
    except Exception:
        summary = {"total_usd": 0}

    # AI í”„ë¡œë°”ì´ë”ë³„ ë¹„ìš© ì •ë³´
    summary["providers"] = {}
    for name, info in AI_PROVIDERS.items():
        summary["providers"][name] = {
            "model": info["model"],
            "cost_tier": info["cost"],
        }
    return jsonify(summary)


# â”€â”€ AI í”„ë¡œë°”ì´ë” ëª©ë¡ â”€â”€
@app.route('/api/ai/providers')
def ai_providers():
    return jsonify(ai_service.list_providers())


# â”€â”€ AI ì§ì ‘ í˜¸ì¶œ (í…ŒìŠ¤íŠ¸/ë‹¨ë… ì‚¬ìš©) â”€â”€
@app.route('/api/ai/ask', methods=['POST'])
def ai_ask():
    data = request.json or {}
    prompt = data.get("prompt", "").strip()
    provider = data.get("provider")  # Noneì´ë©´ ìë™ í´ë°±
    if not prompt:
        return jsonify({"error": "prompt í•„ìˆ˜"}), 400

    try:
        resp = ai_service.ask(prompt, provider=provider)
        return jsonify({
            "text": resp.text,
            "provider": resp.provider,
            "model": resp.model,
            "tokens": {"input": resp.input_tokens, "output": resp.output_tokens},
            "cost_usd": resp.cost_usd,
            "elapsed_ms": resp.elapsed_ms,
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ìº í˜ì¸ íˆìŠ¤í† ë¦¬ â”€â”€
@app.route('/api/campaigns')
def list_campaigns():
    """ìµœê·¼ ìº í˜ì¸ ì´ë ¥ ì¡°íšŒ"""
    limit = request.args.get("limit", 20, type=int)
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.row_factory = sqlite3.Row
    rows = conn.execute(
        "SELECT * FROM campaigns ORDER BY created_at DESC LIMIT ?", (limit,)
    ).fetchall()
    conn.close()
    return jsonify([dict(r) for r in rows])


@app.route('/api/campaigns/<campaign_id>')
def get_campaign(campaign_id):
    """íŠ¹ì • ìº í˜ì¸ ìƒì„¸ ì¡°íšŒ"""
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.row_factory = sqlite3.Row
    row = conn.execute("SELECT * FROM campaigns WHERE id = ?", (campaign_id,)).fetchone()
    conn.close()
    if not row:
        return jsonify({"error": "ìº í˜ì¸ ì—†ìŒ"}), 404
    result = dict(row)
    # ê²°ê³¼ JSON íŒŒì‹±
    if result.get("results"):
        try:
            result["results"] = json.loads(result["results"])
        except Exception:
            pass
    return jsonify(result)


def _save_campaign(campaign_id, topic, brand, platforms, status, results=None, cost=0.0):
    """ìº í˜ì¸ ì´ë ¥ DB ì €ì¥"""
    conn = sqlite3.connect(CAMPAIGN_DB)
    conn.execute("""INSERT OR REPLACE INTO campaigns
        (id, topic, brand, platforms, status, results, cost_usd, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
        (campaign_id, topic, brand, json.dumps(platforms),
         status, json.dumps(results) if results else None,
         cost, datetime.now().isoformat())
    )
    conn.commit()
    conn.close()


# â”€â”€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ë¯¸ë¦¬ë³´ê¸° (ë Œë”ë§ëœ ì˜ìƒ/ì´ë¯¸ì§€) â”€â”€
@app.route('/api/file/<path:filepath>')
def serve_file(filepath):
    full_path = PROJECT_DIR / filepath
    if full_path.exists() and full_path.is_file():
        # MIME íƒ€ì… ìë™ ê°ì§€ + ë¹„ë””ì˜¤/ì´ë¯¸ì§€ëŠ” inline í‘œì‹œ
        suffix = full_path.suffix.lower()
        mime_map = {
            '.mp4': 'video/mp4', '.webm': 'video/webm', '.avi': 'video/x-msvideo',
            '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png',
            '.gif': 'image/gif', '.webp': 'image/webp',
        }
        mimetype = mime_map.get(suffix)
        return send_file(str(full_path), mimetype=mimetype)
    return jsonify({"error": "íŒŒì¼ ì—†ìŒ"}), 404


# â”€â”€ ë Œë”ë§ ì¶œë ¥ í´ë” ì§ì ‘ ëª©ë¡ (ë””ë²„ê¹…ìš©) â”€â”€
@app.route('/api/renders')
def list_renders():
    renders_dir = PROJECT_DIR / "affiliate_system" / "renders"
    if not renders_dir.exists():
        return jsonify({"files": []})
    files = []
    for f in sorted(renders_dir.iterdir(), key=lambda x: x.stat().st_mtime, reverse=True):
        if f.is_file():
            files.append({
                "name": f.name,
                "size_mb": round(f.stat().st_size / (1024*1024), 2),
                "url": f"/api/file/affiliate_system/renders/{f.name}",
                "modified": datetime.fromtimestamp(f.stat().st_mtime).isoformat(),
            })
    return jsonify({"files": files[:50]})


# â”€â”€ Google Drive ìƒíƒœ â”€â”€
@app.route('/api/drive/status')
def drive_status():
    """Google Drive ì¸ì¦ ìƒíƒœ ë° ì €ì¥ìš©ëŸ‰ í™•ì¸"""
    try:
        from affiliate_system.drive_manager import DriveArchiver
        archiver = DriveArchiver()
        token_path = archiver.TOKEN_PATH
        token_exists = token_path.exists()

        if token_exists and archiver.authenticate():
            usage = archiver.get_storage_usage()
            return jsonify({
                "authenticated": True,
                "token_path": str(token_path),
                "storage": usage,
            })
        else:
            return jsonify({
                "authenticated": False,
                "token_path": str(token_path),
                "token_exists": token_exists,
                "error": "ì¸ì¦ í•„ìš”" if not token_exists else "í† í° ë§Œë£Œ",
            })
    except Exception as e:
        return jsonify({"authenticated": False, "error": str(e)})


@app.route('/api/drive/campaigns')
def drive_campaigns():
    """Google Driveì— ì•„ì¹´ì´ë¹™ëœ ìº í˜ì¸ ëª©ë¡"""
    try:
        from affiliate_system.drive_manager import DriveArchiver
        archiver = DriveArchiver()
        if archiver.authenticate():
            campaigns = archiver.list_campaigns()
            return jsonify({"campaigns": campaigns})
        return jsonify({"error": "Drive ì¸ì¦ ì‹¤íŒ¨"}), 401
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# â”€â”€ ìº í˜ì¸ ìƒíƒœ ì¡°íšŒ â”€â”€
@app.route('/api/campaign/status/<job_id>')
def campaign_status(job_id):
    job = jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404

    resp = {
        "job_id": job_id,
        "status": job["status"],
        "topic": job["topic"],
        "brand": job["brand"],
        "platforms": job["platforms"],
        "created_at": job["created_at"],
        "error": job.get("error"),
    }
    if job["status"] == "complete" and job["results"]:
        resp["results"] = _safe_serialize(job["results"])
    return jsonify(resp)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# V2 â€” ëŒ€í™”í˜• ì¿ íŒ¡ ìˆ˜ìµ ê·¹ëŒ€í™” íŒŒì´í”„ë¼ì¸ API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# V2 Job ì €ì¥ì†Œ (interactive ìƒíƒœë¨¸ì‹ )
v2_jobs = {}  # job_id -> {state, coupang_link, product, draft, events, ...}


class V2PipelineState:
    """V2 ëŒ€í™”í˜• íŒŒì´í”„ë¼ì¸ ìƒíƒœ enum."""
    IDLE = "idle"
    AWAITING_LINK = "awaiting_link"
    ANALYZING = "analyzing"
    AWAITING_CONFIRM = "awaiting_confirm"
    EXECUTING = "executing"
    COMPLETE = "complete"
    ERROR = "error"


# V2 10ë‹¨ê³„ ì •ì˜
V2_STEPS = [
    {"step": 1,  "name": "prep_report",    "label": "ì¤€ë¹„ ë¦¬í¬íŠ¸",           "module": "pipeline"},
    {"step": 2,  "name": "link_analysis",   "label": "ì¿ íŒ¡ ë§í¬ ë¶„ì„",       "module": "coupang_scraper"},
    {"step": 3,  "name": "ai_content",      "label": "AI ì½˜í…ì¸  ìƒì„±",       "module": "ai_generator V2"},
    {"step": 4,  "name": "media_crawl",     "label": "ë¯¸ë””ì–´ í¬ë¡¤ë§",         "module": "OmniMediaCollector"},
    {"step": 5,  "name": "blog_compose",    "label": "ë¸”ë¡œê·¸ HTML ì¡°ë¦½",      "module": "blog_html_generator"},
    {"step": 6,  "name": "video_launder",   "label": "4ë‹¨ê³„ ì˜ìƒ ì„¸íƒ",       "module": "video_launderer (FFmpeg GPU)"},
    {"step": 7,  "name": "shorts_render",   "label": "ìˆí¼ ë Œë”ë§",          "module": "ShortsRenderer + TTS + Whisper"},
    {"step": 8,  "name": "thumbnail",       "label": "ì¸ë„¤ì¼ ìƒì„±",          "module": "thumbnail_generator"},
    {"step": 9,  "name": "upload_ready",    "label": "ì—…ë¡œë“œ ì¤€ë¹„",          "module": "auto_uploader V2"},
    {"step": 10, "name": "drive_archive",   "label": "Drive ì•„ì¹´ì´ë¹™",       "module": "drive_manager"},
]


@app.route('/api/v2/steps')
def v2_steps():
    """V2 10ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì •ì˜ ë°˜í™˜."""
    return jsonify(V2_STEPS)


@app.route('/api/v2/campaign/start', methods=['POST'])
def v2_start_campaign():
    """V2 ëŒ€í™”í˜• ìº í˜ì¸ ì‹œì‘ â€” "ì¿ íŒ¡ ë§í¬ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”" ìƒíƒœë¡œ ì§„ì…."""
    job_id = uuid.uuid4().hex[:12]
    events_queue = Queue()

    v2_jobs[job_id] = {
        "state": V2PipelineState.AWAITING_LINK,
        "coupang_link": None,
        "product_info": None,
        "draft": None,
        "blog_html": None,
        "shorts_script": None,
        "results": {},
        "error": None,
        "events": events_queue,
        "created_at": datetime.now().isoformat(),
        "platforms": ["naver_blog", "youtube", "instagram"],
    }

    events_queue.put({
        "type": "state_change",
        "state": V2PipelineState.AWAITING_LINK,
        "message": "ğŸ”— ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”!",
        "timestamp": datetime.now().isoformat(),
    })

    return jsonify({
        "job_id": job_id,
        "state": V2PipelineState.AWAITING_LINK,
        "message": "ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”",
    })


@app.route('/api/v2/campaign/<job_id>/link', methods=['POST'])
def v2_submit_link(job_id):
    """V2 ì¿ íŒ¡ ë§í¬ ì œì¶œ â†’ ìƒí’ˆ ë¶„ì„ ì‹œì‘."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    if job["state"] != V2PipelineState.AWAITING_LINK:
        return jsonify({"error": f"í˜„ì¬ ìƒíƒœ: {job['state']}, ë§í¬ ì…ë ¥ ë¶ˆê°€"}), 400

    data = request.json or {}
    coupang_link = data.get("coupang_link", "").strip()       # ìƒí’ˆì •ë³´ URL (ìŠ¤í¬ë˜í•‘ìš©)
    affiliate_link = data.get("affiliate_link", "").strip()   # ë‹¨ì¶• URL (ìˆ˜ìµ ë§í¬)
    banner_tag = data.get("banner_tag", "").strip()           # ì¿ íŒ¡ ë°°ë„ˆ ì½”ë“œ (<a><img> ë˜ëŠ” iframe)
    product_name = data.get("product_name", "").strip()

    # ë””ë²„ê·¸ ë¡œê·¸ íŒŒì¼ ê¸°ë¡
    from pathlib import Path as _Path
    _dbg = _Path(__file__).parent.parent / "affiliate_system" / "workspace" / "v2_debug.log"
    with open(_dbg, "w", encoding="utf-8") as _f:
        _f.write(f"[SUBMIT] coupang_link={coupang_link[:80]}\n")
        _f.write(f"[SUBMIT] affiliate_link={affiliate_link}\n")
        _f.write(f"[SUBMIT] banner_tag_len={len(banner_tag)}\n")
        _f.write(f"[SUBMIT] banner_tag={banner_tag[:200]}\n")
        _f.write(f"[SUBMIT] product_name_input={product_name}\n")

    # ë°°ë„ˆì½”ë“œ alt ì†ì„±ì—ì„œ ìƒí’ˆëª… ìë™ ì¶”ì¶œ (ì‚¬ìš©ìê°€ ìƒí’ˆëª… ë¯¸ì…ë ¥ ì‹œ)
    if not product_name and banner_tag:
        import re as _re
        _alt_match = _re.search(r'alt=["\']([^"\']+)["\']', banner_tag)
        if _alt_match:
            product_name = _alt_match.group(1).strip()
            with open(_dbg, "a", encoding="utf-8") as _f:
                _f.write(f"[ALT_EXTRACT] product_name={product_name}\n")
        else:
            with open(_dbg, "a", encoding="utf-8") as _f:
                _f.write(f"[ALT_EXTRACT] NO MATCH in banner_tag\n")
    elif product_name:
        with open(_dbg, "a", encoding="utf-8") as _f:
            _f.write(f"[ALT_EXTRACT] SKIPPED - product_name already set: {product_name}\n")
    else:
        with open(_dbg, "a", encoding="utf-8") as _f:
            _f.write(f"[ALT_EXTRACT] SKIPPED - no banner_tag\n")

    if not coupang_link:
        return jsonify({"error": "ìƒí’ˆì •ë³´ ë§í¬ í•„ìˆ˜"}), 400
    if not affiliate_link:
        return jsonify({"error": "ë‹¨ì¶• URL í•„ìˆ˜"}), 400

    job["coupang_link"] = coupang_link
    job["affiliate_link"] = affiliate_link
    job["banner_tag"] = banner_tag
    job["product_name"] = product_name
    job["state"] = V2PipelineState.ANALYZING
    job["events"].put({
        "type": "state_change",
        "state": V2PipelineState.ANALYZING,
        "message": "ğŸ” ìƒí’ˆ ë¶„ì„ ì¤‘...",
        "timestamp": datetime.now().isoformat(),
    })

    # ë¹„ë™ê¸° ë¶„ì„
    def analyze():
        try:
            # Step 1: ì¤€ë¹„
            job["events"].put({
                "type": "v2_step", "step": 1, "name": "prep_report",
                "status": "complete", "detail": "V2 íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ",
                "timestamp": datetime.now().isoformat(),
            })

            # Step 2: ë§í¬ ë¶„ì„
            job["events"].put({
                "type": "v2_step", "step": 2, "name": "link_analysis",
                "status": "running", "detail": "ì¿ íŒ¡ ë§í¬ ìŠ¤í¬ë˜í•‘ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })

            from affiliate_system.pipeline import ContentPipeline
            from affiliate_system.models import Product
            pipeline = ContentPipeline()
            product = pipeline._prepare_product(coupang_link)

            # ë””ë²„ê·¸ ë¡œê·¸ â€” ìŠ¤í¬ë˜í•‘ ê²°ê³¼ + í´ë°± íŒë‹¨
            _dbg = _Path(__file__).parent.parent / "affiliate_system" / "workspace" / "v2_debug.log"
            with open(_dbg, "a", encoding="utf-8") as _f:
                _f.write(f"[SCRAPE] product.title={product.title}\n")
                _f.write(f"[SCRAPE] product.description={str(product.description)[:100]}\n")
                _f.write(f"[SCRAPE] product_name_var={product_name}\n")

            # ì¿ íŒ¡ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ë°°ë„ˆì½”ë“œ alt â†’ ì‚¬ìš©ì ì…ë ¥ ìƒí’ˆëª… ìˆœìœ¼ë¡œ í´ë°±
            _bad_titles = ("ì¿ íŒ¡ ìƒí’ˆ", "ì¸ê¸°ìƒí’ˆ", "", None)
            if not product.title or product.title in _bad_titles:
                # 1ì°¨ í´ë°±: ì‚¬ìš©ì ì…ë ¥ ë˜ëŠ” ë°°ë„ˆì½”ë“œ altì—ì„œ ì¶”ì¶œëœ ìƒí’ˆëª…
                if product_name:
                    with open(_dbg, "a", encoding="utf-8") as _f:
                        _f.write(f"[FALLBACK] Using product_name: {product_name}\n")
                    product = Product(
                        title=product_name,
                        description=f"{product_name} - ì¿ íŒ¡ ìµœì €ê°€ ìƒí’ˆ",
                        url=coupang_link,
                        affiliate_link=affiliate_link,
                        scraped_at=product.scraped_at,
                    )
                else:
                    with open(_dbg, "a", encoding="utf-8") as _f:
                        _f.write(f"[FALLBACK] WARNING: No product_name, using default\n")
            elif product_name and product.title != product_name:
                # ìŠ¤í¬ë˜í•‘ ì„±ê³µí–ˆì§€ë§Œ ë°°ë„ˆ altì™€ ë‹¤ë¥¸ ê²½ìš° â†’ ë°°ë„ˆ alt ìš°ì„  (ë” ì •í™•)
                print(f"[V2] ë°°ë„ˆì½”ë“œ ìƒí’ˆëª…ìœ¼ë¡œ êµì²´: {product.title} â†’ {product_name}")
                product = Product(
                    title=product_name,
                    description=product.description or f"{product_name} - ì¿ íŒ¡ ìµœì €ê°€",
                    url=coupang_link,
                    affiliate_link=affiliate_link,
                    scraped_at=product.scraped_at,
                    price=getattr(product, 'price', ''),
                    images=getattr(product, 'images', []),
                )

            # í•­ìƒ ìˆ˜ìµ ë§í¬ë¥¼ íŒŒíŠ¸ë„ˆìŠ¤ ë§í¬ë¡œ ì„¤ì •
            product.affiliate_link = affiliate_link

            product_info = {
                "title": product.title,
                "description": product.description or "",
                "price": product.price or "",
                "image_urls": product.image_urls[:3] if product.image_urls else [],
                "affiliate_link": affiliate_link,  # ìˆ˜ìµ ë§í¬ (link.coupang.com/a/...)
                "product_url": coupang_link,        # ìƒí’ˆì •ë³´ ë§í¬ (coupang.com/vp/products/...)
            }
            job["product_info"] = product_info

            job["events"].put({
                "type": "v2_step", "step": 2, "name": "link_analysis",
                "status": "complete",
                "detail": f"ìƒí’ˆ: {product.title}",
                "timestamp": datetime.now().isoformat(),
            })

            # Step 3: AI ì½˜í…ì¸  ì´ˆì•ˆ ìƒì„±
            job["events"].put({
                "type": "v2_step", "step": 3, "name": "ai_content",
                "status": "running", "detail": "ë¸”ë¡œê·¸ + ìˆí¼ ëŒ€ë³¸ AI ìƒì„± ì¤‘ (Gemini ë¬´ë£Œ)...",
                "timestamp": datetime.now().isoformat(),
            })

            try:
                from affiliate_system.ai_generator import AIGenerator
                generator = AIGenerator()
                # ë””ë²„ê·¸: AI ìƒì„± ì§ì „ ìµœì¢… product.title í™•ì¸
                with open(_dbg, "a", encoding="utf-8") as _f:
                    _f.write(f"[AI_GEN] FINAL product.title={product.title}\n")
                    _f.write(f"[AI_GEN] FINAL product.description={str(product.description)[:100]}\n")

                # V2 ë¸”ë¡œê·¸ ì½˜í…ì¸  â€” ìˆ˜ìµ ë§í¬ë¡œ ìƒì„±
                blog_content = generator.generate_blog_content_v2(product, affiliate_link)
                job["draft"] = {
                    "blog": blog_content,
                    "product": product_info,
                }

                # V2 ìˆí¼ í›„í‚¹ ëŒ€ë³¸
                try:
                    shorts_scenes = generator.generate_shorts_hooking_script(
                        product, persona="", coupang_link=affiliate_link, dm_keyword="ë§í¬"
                    )
                    # list[dict] â†’ {"scenes": [...]} í˜•íƒœë¡œ ê°ì‹¸ê¸° (Step 7 í˜¸í™˜)
                    if isinstance(shorts_scenes, list):
                        shorts_script = {"scenes": shorts_scenes}
                    elif isinstance(shorts_scenes, dict) and "scenes" in shorts_scenes:
                        shorts_script = shorts_scenes
                    else:
                        shorts_script = {"scenes": []}
                    job["shorts_script"] = shorts_script
                    job["draft"]["shorts"] = shorts_script
                    print(f"[V2] ìˆí¼ ëŒ€ë³¸: {len(shorts_script.get('scenes', []))}ì¥ë©´ ìƒì„± ì™„ë£Œ")
                except Exception as se:
                    import traceback
                    print(f"[V2] ìˆí¼ ëŒ€ë³¸ ìƒì„± ì‹¤íŒ¨: {se}")
                    traceback.print_exc()
                    job["draft"]["shorts"] = {"error": str(se)}

                job["events"].put({
                    "type": "v2_step", "step": 3, "name": "ai_content",
                    "status": "complete",
                    "detail": f"ë¸”ë¡œê·¸ {len(blog_content.get('body_sections', []))}ì„¹ì…˜ + ìˆí¼ ëŒ€ë³¸ ìƒì„± ì™„ë£Œ",
                    "timestamp": datetime.now().isoformat(),
                })

            except Exception as ai_err:
                job["events"].put({
                    "type": "v2_step", "step": 3, "name": "ai_content",
                    "status": "error", "detail": str(ai_err),
                    "timestamp": datetime.now().isoformat(),
                })

            # í™•ì¸ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜
            job["state"] = V2PipelineState.AWAITING_CONFIRM
            job["events"].put({
                "type": "state_change",
                "state": V2PipelineState.AWAITING_CONFIRM,
                "message": "âœ… ë¶„ì„ ì™„ë£Œ! ì´ˆì•ˆì„ í™•ì¸í•˜ê³  ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.",
                "draft": job["draft"],
                "timestamp": datetime.now().isoformat(),
            })

        except Exception as e:
            job["state"] = V2PipelineState.ERROR
            job["error"] = str(e)
            job["events"].put({
                "type": "error", "error": str(e),
                "timestamp": datetime.now().isoformat(),
            })

    thread = threading.Thread(target=analyze, daemon=True)
    thread.start()

    return jsonify({"job_id": job_id, "state": V2PipelineState.ANALYZING})


@app.route('/api/v2/campaign/<job_id>/confirm', methods=['POST'])
def v2_confirm_execute(job_id):
    """V2 ì‹¤í–‰ í™•ì¸ â†’ ë‚˜ë¨¸ì§€ 7ë‹¨ê³„ (ë¯¸ë””ì–´ í¬ë¡¤ë§ ~ Drive ì•„ì¹´ì´ë¹™) ì‹¤í–‰."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    if job["state"] != V2PipelineState.AWAITING_CONFIRM:
        return jsonify({"error": f"í˜„ì¬ ìƒíƒœ: {job['state']}, ì‹¤í–‰ í™•ì¸ ë¶ˆê°€"}), 400

    # í”Œë«í¼ë³„ ì—…ë¡œë“œ í† ê¸€ ì €ì¥
    confirm_data = request.json or {}
    job["upload_youtube"] = confirm_data.get("upload_youtube", False)
    job["upload_instagram"] = confirm_data.get("upload_instagram", False)
    job["upload_naver"] = confirm_data.get("upload_naver", False)

    job["state"] = V2PipelineState.EXECUTING
    job["events"].put({
        "type": "state_change",
        "state": V2PipelineState.EXECUTING,
        "message": "ğŸš€ ì‹¤í–‰ ì‹œì‘! 10ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì§„í–‰ ì¤‘...",
        "timestamp": datetime.now().isoformat(),
    })

    def execute():
        try:
            coupang_link = job["coupang_link"]
            affiliate_link = job.get("affiliate_link", coupang_link)  # ìˆ˜ìµ ë§í¬
            banner_tag = job.get("banner_tag", "")  # ì¿ íŒ¡ ë°°ë„ˆ ì½”ë“œ
            draft = job.get("draft", {})
            blog_content = draft.get("blog", {})
            product_info = job.get("product_info", {})
            product_title = product_info.get("title", "ìƒí’ˆ")

            # Step 4: ìŠ¤ë§ˆíŠ¸ ë¯¸ë””ì–´ í¬ë¡¤ë§ + AI ì´ë¯¸ì§€ ìƒì„±
            job["events"].put({
                "type": "v2_step", "step": 4, "name": "media_crawl",
                "status": "running", "detail": "Gemini í‚¤ì›Œë“œ ë¶„ì„ + ë¯¸ë””ì–´ ìˆ˜ì§‘ + AI ì´ë¯¸ì§€ ìƒì„±...",
                "timestamp": datetime.now().isoformat(),
            })
            blog_images = []
            video_sources = []
            ai_images = []
            try:
                from affiliate_system.media_collector import OmniMediaCollector, MediaCollector
                from affiliate_system.ai_generator import AIGenerator
                omni = OmniMediaCollector()
                gen = AIGenerator()

                # â”€â”€ Gemini SmartMediaMatcher: ì£¼ì œ ë¶„ì„ â†’ ìµœì  í‚¤ì›Œë“œ ìƒì„± â”€â”€
                product_features = product_info.get("features", "")
                if isinstance(product_features, list):
                    product_features = ", ".join(product_features)
                category = product_info.get("category", "")
                smart_keywords = gen.generate_smart_media_keywords(
                    product_name=product_title,
                    category=category,
                    product_features=product_features,
                )
                job["smart_keywords"] = smart_keywords
                job["category"] = smart_keywords.get("category_detected", category)
                job["product_name"] = product_title

                # ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
                image_kw_en = smart_keywords.get("image_keywords_en", [product_title])
                image_kw_ko = smart_keywords.get("image_keywords_ko", [product_title])
                all_image_kw = image_kw_en + image_kw_ko
                product_image_urls = product_info.get("image_urls", [])

                blog_images = omni.collect_blog_images(
                    product_title=product_title,
                    image_keywords=all_image_kw[:7],
                    product_image_urls=product_image_urls,
                    count=5,
                )

                # ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œë¡œ ë¹„ë””ì˜¤ ê²€ìƒ‰
                video_kw_en = smart_keywords.get("video_keywords_en", [])
                search_en = video_kw_en[0] if video_kw_en else gen.translate_for_search(product_title)
                try:
                    video_sources = omni.collect_video_sources(
                        product_title=product_title,
                        search_keyword_en=search_en,
                        count=6,
                    )
                except Exception:
                    video_sources = []

                # â”€â”€ Gemini Imagen 4.0: AI ì´ë¯¸ì§€ ìƒì„± (ë¶€ì¡±ë¶„ ë³´ì¶© + ê³ í€„ CTA) â”€â”€
                ai_prompts = smart_keywords.get("ai_image_prompts", [])
                if ai_prompts:
                    try:
                        from affiliate_system.config import V2_BLOG_DIR
                        ai_images = gen.generate_ai_images(
                            prompts=ai_prompts[:3],
                            output_dir=str(V2_BLOG_DIR / "ai_generated"),
                            count_per_prompt=1,
                            aspect_ratio="9:16",
                        )
                        # AI ì´ë¯¸ì§€ë¥¼ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ í’€ì— ì¶”ê°€
                        blog_images.extend(ai_images)
                    except Exception as ai_err:
                        print(f"[V2] AI ì´ë¯¸ì§€ ìƒì„± ìŠ¤í‚µ: {ai_err}")

                job["events"].put({
                    "type": "v2_step", "step": 4, "name": "media_crawl",
                    "status": "complete",
                    "detail": (
                        f"í¬ë¡¤ë§ ì´ë¯¸ì§€ {len(blog_images)-len(ai_images)}ì¥ + "
                        f"AI ì´ë¯¸ì§€ {len(ai_images)}ì¥ + "
                        f"ì˜ìƒ {len(video_sources)}ê°œ ìˆ˜ì§‘"
                    ),
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as me:
                import traceback
                print(f"[V2] Step 4 ë¯¸ë””ì–´ í¬ë¡¤ë§ ì—ëŸ¬: {me}")
                print(traceback.format_exc())
                job["events"].put({
                    "type": "v2_step", "step": 4, "name": "media_crawl",
                    "status": "error", "detail": str(me),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 5: ë¸”ë¡œê·¸ HTML ì¡°ë¦½
            job["events"].put({
                "type": "v2_step", "step": 5, "name": "blog_compose",
                "status": "running", "detail": "ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ êµì°¨ ë°°ì¹˜ HTML ìƒì„± ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            blog_html = ""
            try:
                from affiliate_system.blog_html_generator import NaverBlogHTMLGenerator
                html_gen = NaverBlogHTMLGenerator()
                blog_html = html_gen.generate_blog_html(
                    title=blog_content.get("title", product_info.get("title", "")),
                    intro=blog_content.get("intro", ""),
                    body_sections=blog_content.get("body_sections", []),
                    image_paths=[p for p in blog_images if p],  # ë¹ˆ ê²½ë¡œ í•„í„°ë§
                    coupang_link=affiliate_link,  # ìˆ˜ìµ ë§í¬ ì‚¬ìš©!
                    hashtags=blog_content.get("hashtags", []),
                    banner_tag=banner_tag,  # ì¿ íŒ¡ ë°°ë„ˆ ì½”ë“œ
                )
                job["blog_html"] = blog_html
                job["events"].put({
                    "type": "v2_step", "step": 5, "name": "blog_compose",
                    "status": "complete",
                    "detail": f"HTML {len(blog_html)}ì ìƒì„± (ì´ë¯¸ì§€ {len(blog_images)}ì¥ êµì°¨ ë°°ì¹˜)",
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as he:
                job["events"].put({
                    "type": "v2_step", "step": 5, "name": "blog_compose",
                    "status": "error", "detail": str(he),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 6: ì˜ìƒ ì„¸íƒ
            job["events"].put({
                "type": "v2_step", "step": 6, "name": "video_launder",
                "status": "running", "detail": "4ë‹¨ê³„ FFmpeg GPU ì„¸íƒ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            laundered_videos = []
            try:
                if video_sources:
                    from affiliate_system.video_launderer import VideoLaunderer
                    launderer = VideoLaunderer()
                    video_paths = [v["path"] for v in video_sources if v.get("path")]
                    laundered_videos = launderer.batch_launder(video_paths)
                    job["events"].put({
                        "type": "v2_step", "step": 6, "name": "video_launder",
                        "status": "complete",
                        "detail": f"{len(laundered_videos)}ê°œ ì˜ìƒ ì„¸íƒ ì™„ë£Œ",
                        "timestamp": datetime.now().isoformat(),
                    })
                else:
                    # ë¹„ë””ì˜¤ ì†ŒìŠ¤ ì—†ìŒ â†’ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ë¥¼ ì˜ìƒ í´ë¦½ìœ¼ë¡œ ë³€í™˜ (Ken Burns)
                    if blog_images:
                        import subprocess
                        from affiliate_system.config import V2_SHORTS_DIR, FFMPEG_CRF
                        _img_vid_dir = _Path(V2_SHORTS_DIR) / "img_clips"
                        _img_vid_dir.mkdir(parents=True, exist_ok=True)

                        for img_i, img_path in enumerate(blog_images[:6]):
                            try:
                                out_clip = str(_img_vid_dir / f"img_clip_{img_i}_{job_id[:8]}.mp4")
                                # FFmpeg: ì´ë¯¸ì§€ â†’ 8ì´ˆ ì˜ìƒ (zoompan Ken Burns íš¨ê³¼)
                                subprocess.run([
                                    "ffmpeg", "-y",
                                    "-loop", "1", "-i", str(img_path),
                                    "-vf", (
                                        "scale=1080:1920:force_original_aspect_ratio=increase,"
                                        "crop=1080:1920,"
                                        "zoompan=z='min(zoom+0.0015,1.3)':d=240:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':s=1080x1920:fps=30"
                                    ),
                                    "-t", "8",
                                    "-c:v", "libx264",
                                    "-crf", FFMPEG_CRF,
                                    "-preset", "medium",
                                    "-pix_fmt", "yuv420p",
                                    "-an",  # ì˜¤ë””ì˜¤ ì—†ìŒ
                                    out_clip,
                                ], capture_output=True, timeout=60)

                                if os.path.exists(out_clip) and os.path.getsize(out_clip) > 10000:
                                    laundered_videos.append(out_clip)
                            except Exception as _img_err:
                                print(f"[V2] ì´ë¯¸ì§€â†’ì˜ìƒ ë³€í™˜ ì‹¤íŒ¨ [{img_i}]: {_img_err}")

                        print(f"[V2] ì´ë¯¸ì§€â†’ì˜ìƒ í´ë°±: {len(laundered_videos)}ê°œ ìƒì„±")

                    job["events"].put({
                        "type": "v2_step", "step": 6, "name": "video_launder",
                        "status": "complete",
                        "detail": f"ì´ë¯¸ì§€â†’ì˜ìƒ í´ë°±: {len(laundered_videos)}ê°œ í´ë¦½ ìƒì„±" if laundered_videos else "ì˜ìƒ/ì´ë¯¸ì§€ ì—†ìŒ",
                        "timestamp": datetime.now().isoformat(),
                    })
            except Exception as le:
                job["events"].put({
                    "type": "v2_step", "step": 6, "name": "video_launder",
                    "status": "error", "detail": str(le),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 7: ìˆí¼ ë Œë”ë§
            job["events"].put({
                "type": "v2_step", "step": 7, "name": "shorts_render",
                "status": "running", "detail": "TTS + ìë§‰ ì‹±í¬ + ìˆí¼ ì¡°ë¦½ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            shorts_path = None
            _dbg_log = Path(WORK_DIR) / "step7_debug.log"
            try:
                _dbg = f"Step 7 ì²´í¬: lv={len(laundered_videos) if laundered_videos else 0}, script={type(job.get('shorts_script'))}, has_script={bool(job.get('shorts_script'))}\n"
                _dbg_log.write_text(_dbg, encoding="utf-8")
                job["results"]["step7_debug"] = _dbg.strip()
                if laundered_videos and job.get("shorts_script"):
                    from affiliate_system.video_launderer import (
                        EmotionTTSEngine, SubtitleGenerator, ShortsRenderer,
                        ProShortsRenderer, _detect_bgm_genre,
                    )
                    from affiliate_system.config import V2_TTS_DIR, V2_SUBTITLE_DIR, V2_SHORTS_DIR

                    script = job["shorts_script"]
                    # list ë˜ëŠ” {"scenes": [...]} ë‘˜ ë‹¤ ì§€ì›
                    if isinstance(script, list):
                        scenes_data = script
                    elif isinstance(script, dict):
                        scenes_data = script.get("scenes", [])
                    else:
                        scenes_data = []

                    _dbg_log.write_text(f"Step7 ì§„ì…: scenes={len(scenes_data)}, lv={len(laundered_videos)}\n", encoding="utf-8")

                    # emotion ìœ íš¨ì„± ê²€ì¦
                    valid_emotions = {"excited", "friendly", "urgent", "dramatic", "calm", "hyped"}
                    for sd in scenes_data:
                        emo = sd.get("emotion", "friendly")
                        if emo not in valid_emotions:
                            sd["emotion"] = "friendly"

                    # TTS ìƒì„±
                    _dbg_log.write_text(f"TTS ì‹œì‘...\n", encoding="utf-8")
                    tts_engine = EmotionTTSEngine()
                    scenes = tts_engine.generate_scenes_tts(scenes_data, job_id)
                    _dbg_log.write_text(f"TTS ì™„ë£Œ: {len(scenes)}ì¥ë©´\n", encoding="utf-8")

                    # ìë§‰ ìƒì„±
                    sub_gen = SubtitleGenerator()
                    subtitle_path = sub_gen.generate_ass_from_scenes(scenes, job_id)
                    if not subtitle_path:
                        subtitle_path = str(V2_SUBTITLE_DIR / f"{job_id}_subtitle.ass")
                    _dbg_log.write_text(f"ìë§‰: {subtitle_path}\n", encoding="utf-8")

                    # ì„¸íƒëœ ì˜ìƒì„ sceneì— ë§¤í•‘ (round-robin ìˆœí™˜)
                    render_scenes = []
                    for i, sc in enumerate(scenes):
                        video_idx = i % len(laundered_videos)
                        video_path = laundered_videos[video_idx]
                        render_scenes.append({
                            "video_clip_path": video_path,
                            "tts_path": sc.get("tts_path", "") or "",
                            "tts_duration": sc.get("tts_duration", sc.get("duration", 3.0)),
                            "text": sc.get("text", ""),
                            "emotion": sc.get("emotion", "friendly"),
                        })

                    # ìµœì¢… ë Œë”ë§ â€” ProShortsRenderer V3 (ëª¨ì…˜+ì „í™˜+BGM+ì»¬ëŸ¬ê·¸ë ˆì´ë”©)
                    _dbg_log.write_text(f"ProShortsRenderer ì‹œì‘: {len(render_scenes)}ì¥ë©´\n", encoding="utf-8")
                    product_name = job.get("product_name", topic)
                    category = job.get("category", "")
                    try:
                        renderer = ProShortsRenderer()
                        result_path = renderer.render_pro_shorts(
                            scenes=render_scenes,
                            campaign_id=job_id,
                            subtitle_path=subtitle_path,
                            product_name=product_name,
                            category=category,
                        )
                    except Exception as pro_err:
                        _dbg_log.write_text(f"ProShortsRenderer ì‹¤íŒ¨, í´ë°±: {pro_err}\n", encoding="utf-8")
                        renderer = ShortsRenderer()
                        result_path = renderer.render_final_shorts(
                            scenes=render_scenes,
                            campaign_id=job_id,
                            subtitle_path=subtitle_path,
                            coupang_link=affiliate_link,
                        )
                    _dbg_log.write_text(f"ë Œë”ë§ ê²°ê³¼: {result_path}\n", encoding="utf-8")
                    if result_path:
                        shorts_path = result_path

                    job["events"].put({
                        "type": "v2_step", "step": 7, "name": "shorts_render",
                        "status": "complete",
                        "detail": f"ìˆí¼ ë Œë”ë§ ì™„ë£Œ: {Path(shorts_path).name}" if shorts_path else "ë Œë”ë§ ì‹¤íŒ¨",
                        "timestamp": datetime.now().isoformat(),
                    })
                else:
                    skip_reason = f"laundered={len(laundered_videos) if laundered_videos else 0}, script={bool(job.get('shorts_script'))}"
                    job["results"]["shorts_skip"] = skip_reason
                    job["events"].put({
                        "type": "v2_step", "step": 7, "name": "shorts_render",
                        "status": "complete", "detail": f"ìˆí¼ ìŠ¤í‚µ: {skip_reason}",
                        "timestamp": datetime.now().isoformat(),
                    })
            except Exception as render_err:
                import traceback
                err_detail = traceback.format_exc()
                print(f"[V2] Step 7 ìˆí¼ ë Œë”ë§ ì—ëŸ¬: {render_err}")
                print(err_detail)
                job["results"]["shorts_error"] = f"{render_err}\n{err_detail}"
                job["events"].put({
                    "type": "v2_step", "step": 7, "name": "shorts_render",
                    "status": "error", "detail": str(render_err),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 8: ì¸ë„¤ì¼
            job["events"].put({
                "type": "v2_step", "step": 8, "name": "thumbnail",
                "status": "running", "detail": "í”Œë«í¼ë³„ ì¸ë„¤ì¼ ìƒì„± ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            try:
                # ì¸ë„¤ì¼ì€ V1 íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
                job["events"].put({
                    "type": "v2_step", "step": 8, "name": "thumbnail",
                    "status": "complete", "detail": "ì¸ë„¤ì¼ ìƒì„± ì™„ë£Œ (ë˜ëŠ” ìƒëµ)",
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as te:
                job["events"].put({
                    "type": "v2_step", "step": 8, "name": "thumbnail",
                    "status": "error", "detail": str(te),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 9: í”Œë«í¼ë³„ ìë™ ì—…ë¡œë“œ (ON/OFF ìŠ¤ìœ„ì¹˜ ê¸°ë°˜)
            upload_youtube = job.get("upload_youtube", False)
            upload_instagram = job.get("upload_instagram", False)
            upload_naver = job.get("upload_naver", False)
            any_upload = upload_youtube or upload_instagram or upload_naver

            job["events"].put({
                "type": "v2_step", "step": 9, "name": "upload_ready",
                "status": "running",
                "detail": f"ì—…ë¡œë“œ: YT={'ON' if upload_youtube else 'OFF'} | IG={'ON' if upload_instagram else 'OFF'} | Blog={'ON' if upload_naver else 'OFF'}",
                "timestamp": datetime.now().isoformat(),
            })
            upload_results = {}
            try:
                job["results"]["blog_html"] = blog_html
                job["results"]["blog_images"] = blog_images
                job["results"]["shorts_path"] = shorts_path
                job["results"]["laundered_videos"] = laundered_videos

                # í”Œë«í¼ë³„ ìë™ ì—…ë¡œë“œ ì‹¤í–‰
                if any_upload:
                    try:
                        from affiliate_system.auto_uploader import StealthUploader
                        uploader = StealthUploader()
                        uploaded = []

                        if upload_youtube and shorts_path:
                            try:
                                if uploader.youtube_auth():
                                    yt_result = uploader.youtube_upload_v2(
                                        video_path=shorts_path,
                                        title=f"{product_title} ì¶”ì²œ #Shorts",
                                        description=f"#{product_title} #ì¿ íŒ¡ #ì¶”ì²œ #ì‡¼ì¸ ",
                                    )
                                    if yt_result:
                                        uploaded.append("YouTube")
                                        upload_results["youtube"] = yt_result
                            except Exception as yt_err:
                                upload_results["youtube_error"] = str(yt_err)

                        if upload_instagram and shorts_path:
                            try:
                                if uploader.instagram_auth():
                                    ig_result = uploader.instagram_upload_reel_v2(
                                        video_path=shorts_path,
                                        caption=f"{product_title} ì†”ì§ ì¶”ì²œ! ğŸ’¯\n#ì¿ íŒ¡ #{product_title.replace(' ', '')} #ì¶”ì²œ",
                                    )
                                    if ig_result:
                                        uploaded.append("Instagram")
                                        upload_results["instagram"] = ig_result
                            except Exception as ig_err:
                                upload_results["instagram_error"] = str(ig_err)

                        if upload_naver and blog_html:
                            try:
                                naver_result = uploader.naver_blog_post_v2(
                                    html_content=blog_html,
                                    title=product_title,
                                )
                                if naver_result:
                                    uploaded.append("Naver")
                                    upload_results["naver"] = naver_result
                            except Exception as nv_err:
                                upload_results["naver_error"] = str(nv_err)

                        upload_detail = f"ì—…ë¡œë“œ ì™„ë£Œ: {', '.join(uploaded)}" if uploaded else "ì—…ë¡œë“œ ëŒ€ìƒ ì—†ìŒ"
                    except Exception as up_err:
                        upload_detail = f"ì—…ë¡œë” ë¡œë“œ ì‹¤íŒ¨: {up_err}"
                else:
                    upload_detail = "ìë™ ì—…ë¡œë“œ OFF â€” ê²°ê³¼ë¬¼ í™•ì¸ í›„ ìˆ˜ë™ ì—…ë¡œë“œ"

                job["results"]["upload_results"] = upload_results

                job["events"].put({
                    "type": "v2_step", "step": 9, "name": "upload_ready",
                    "status": "complete",
                    "detail": upload_detail,
                    "timestamp": datetime.now().isoformat(),
                })
            except Exception as ue:
                job["events"].put({
                    "type": "v2_step", "step": 9, "name": "upload_ready",
                    "status": "error", "detail": str(ue),
                    "timestamp": datetime.now().isoformat(),
                })

            # Step 10: Drive ì•„ì¹´ì´ë¹™
            job["events"].put({
                "type": "v2_step", "step": 10, "name": "drive_archive",
                "status": "running", "detail": "Google Drive ì•„ì¹´ì´ë¹™ ì¤‘...",
                "timestamp": datetime.now().isoformat(),
            })
            try:
                from affiliate_system.drive_manager import DriveArchiver
                archiver = DriveArchiver()
                if archiver.authenticate():
                    # V2 í”Œë«í¼ë³„ íŒŒì¼ ë¶„ë¥˜ â€” ë°”ë¡œ í´ë¦­í•´ì„œ ë³¼ ìˆ˜ ìˆëŠ” êµ¬ì¡°
                    valid_images = [p for p in blog_images if p and Path(p).exists()]
                    drive_files = {
                        # ë„¤ì´ë²„ë¸”ë¡œê·¸: ë¸”ë¡œê·¸ HTML + ì´ë¯¸ì§€
                        "naver_blog": [],
                        # ì¸ìŠ¤íƒ€ê·¸ë¨ìˆì¸ : ìˆí¼ ì˜ìƒ (ë™ì¼ ì˜ìƒ ê³µìœ )
                        "instagram_shorts": [],
                        # ìœ íŠœë¸Œìˆì¸ : ìˆí¼ ì˜ìƒ (ë™ì¼ ì˜ìƒ ê³µìœ )
                        "youtube_shorts": [],
                    }

                    # ë„¤ì´ë²„ë¸”ë¡œê·¸ í´ë”: HTML + ì´ë¯¸ì§€
                    if blog_html:
                        blog_html_path = Path(WORK_DIR) / f"blog_{job_id}.html"
                        blog_html_path.write_text(blog_html, encoding="utf-8")
                        drive_files["naver_blog"].append(str(blog_html_path))
                    drive_files["naver_blog"].extend(valid_images)

                    # ìˆí¼ ì˜ìƒ â†’ ì¸ìŠ¤íƒ€ê·¸ë¨ + ìœ íŠœë¸Œ ì–‘ìª½ì— ì—…ë¡œë“œ
                    if shorts_path and Path(shorts_path).exists():
                        drive_files["instagram_shorts"].append(shorts_path)
                        drive_files["youtube_shorts"].append(shorts_path)

                    # ì„¸íƒëœ ì›ë³¸ ì˜ìƒë„ ìœ íŠœë¸Œ í´ë”ì— ì¶”ê°€ (í¸ì§‘ìš© ì†ŒìŠ¤)
                    for lv in laundered_videos:
                        if lv and Path(lv).exists():
                            drive_files["youtube_shorts"].append(lv)

                    # ì„ì‹œ Campaign ê°ì²´ ìƒì„± â€” ì¬ìŠ¤í¬ë˜í•‘ ì•Šê³  ì €ì¥ëœ ì •ë³´ ì‚¬ìš©
                    from affiliate_system.models import Campaign, AIContent, CampaignStatus, Product
                    temp_product = Product(
                        title=product_title,
                        description=product_info.get("description", ""),
                        url=coupang_link,
                        affiliate_link=affiliate_link,
                    )
                    temp_campaign = Campaign(
                        id=job_id, product=temp_product,
                        ai_content=AIContent(platform_contents={}),
                        status=CampaignStatus.COMPLETE,
                        target_platforms=[],
                        platform_videos={}, platform_thumbnails={},
                        created_at=datetime.now(),
                    )
                    archive_result = archiver.archive_campaign(
                        temp_campaign, drive_files, v2=True
                    )
                    if archive_result["ok"]:
                        job["results"]["drive_url"] = archive_result.get("folder_url", "")
                        job["results"]["drive_platforms"] = archive_result.get("platform_urls", {})
                        job["events"].put({
                            "type": "v2_step", "step": 10, "name": "drive_archive",
                            "status": "complete",
                            "detail": f"Drive ì•„ì¹´ì´ë¹™ ì™„ë£Œ: {archive_result['files_uploaded']}ê°œ íŒŒì¼ (3 í”Œë«í¼)",
                            "timestamp": datetime.now().isoformat(),
                        })
                    else:
                        job["events"].put({
                            "type": "v2_step", "step": 10, "name": "drive_archive",
                            "status": "error", "detail": "Drive ì—…ë¡œë“œ ì¼ë¶€ ì‹¤íŒ¨",
                            "timestamp": datetime.now().isoformat(),
                        })
                else:
                    job["events"].put({
                        "type": "v2_step", "step": 10, "name": "drive_archive",
                        "status": "error", "detail": "Drive ì¸ì¦ ì‹¤íŒ¨",
                        "timestamp": datetime.now().isoformat(),
                    })
            except Exception as de:
                job["events"].put({
                    "type": "v2_step", "step": 10, "name": "drive_archive",
                    "status": "error", "detail": str(de),
                    "timestamp": datetime.now().isoformat(),
                })

            # ì™„ë£Œ
            job["state"] = V2PipelineState.COMPLETE
            job["events"].put({
                "type": "v2_complete",
                "message": "ğŸ‰ V2 íŒŒì´í”„ë¼ì¸ 10ë‹¨ê³„ ì™„ë£Œ!",
                "results": _safe_serialize(job["results"]),
                "timestamp": datetime.now().isoformat(),
            })

            # ìº í˜ì¸ DB ì €ì¥
            _save_campaign(
                job_id, product_info.get("title", "V2 Campaign"),
                "V2", job["platforms"], "complete",
                results=_safe_serialize(job["results"]),
            )

        except Exception as e:
            job["state"] = V2PipelineState.ERROR
            job["error"] = str(e)
            job["events"].put({
                "type": "error", "error": str(e),
                "timestamp": datetime.now().isoformat(),
            })

    thread = threading.Thread(target=execute, daemon=True)
    thread.start()

    return jsonify({"job_id": job_id, "state": V2PipelineState.EXECUTING})


@app.route('/api/v2/campaign/<job_id>/status')
def v2_campaign_status(job_id):
    """V2 ìº í˜ì¸ ìƒíƒœ ì¡°íšŒ."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404

    return jsonify({
        "job_id": job_id,
        "state": job["state"],
        "coupang_link": job.get("coupang_link"),
        "product_info": job.get("product_info"),
        "draft": job.get("draft"),
        "blog_html": job.get("blog_html", ""),
        "results": _safe_serialize(job.get("results", {})),
        "error": job.get("error"),
        "created_at": job.get("created_at"),
    })


@app.route('/api/v2/campaign/<job_id>/stream')
def v2_stream(job_id):
    """V2 SSE ìŠ¤íŠ¸ë¦¬ë° â€” 10ë‹¨ê³„ ì§„í–‰ìƒí™©."""
    def generate():
        job = v2_jobs.get(job_id)
        if not job:
            yield f"data: {json.dumps({'type': 'error', 'error': 'Job not found'})}\n\n"
            return

        q = job["events"]
        while job["state"] not in (V2PipelineState.COMPLETE, V2PipelineState.ERROR):
            while not q.empty():
                event = q.get_nowait()
                yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"
            time.sleep(0.3)

        # ì”ì—¬ ì´ë²¤íŠ¸ flush
        while not q.empty():
            event = q.get_nowait()
            yield f"data: {json.dumps(event, ensure_ascii=False, default=str)}\n\n"

        # ìµœì¢… ìƒíƒœ
        if job["state"] == V2PipelineState.COMPLETE:
            yield f"data: {json.dumps({'type': 'v2_done', 'results': _safe_serialize(job.get('results', {}))}, ensure_ascii=False, default=str)}\n\n"
        elif job["state"] == V2PipelineState.ERROR:
            yield f"data: {json.dumps({'type': 'error', 'error': job.get('error', 'Unknown error')})}\n\n"

    return Response(generate(), mimetype='text/event-stream',
                    headers={'Cache-Control': 'no-cache', 'X-Accel-Buffering': 'no'})


@app.route('/api/v2/campaign/<job_id>/blog-preview')
def v2_blog_preview(job_id):
    """V2 ë¸”ë¡œê·¸ HTML ë¯¸ë¦¬ë³´ê¸°."""
    job = v2_jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404

    blog_html = job.get("blog_html", "")
    if blog_html:
        return Response(blog_html, mimetype='text/html; charset=utf-8')
    return jsonify({"error": "ë¸”ë¡œê·¸ HTML ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ"}), 404


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„œë²„ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == '__main__':
    import io, sys
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    print("=" * 50)
    print("YJ MCN Automation Dashboard Server")
    print(f"  URL: http://localhost:5001")
    print(f"  Gemini: {'OK' if GEMINI_API_KEY else 'NO'}")
    print(f"  Claude: {'OK' if ANTHROPIC_API_KEY else 'NO'}")
    print(f"  Pexels: {'OK' if PEXELS_API_KEY else 'NO'}")
    print("=" * 50)
    app.run(host='0.0.0.0', port=5001, debug=False, threaded=True)
